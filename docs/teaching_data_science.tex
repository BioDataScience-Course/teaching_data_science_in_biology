% !!!IMPORTANT NOTE: Please read carefully all information including those preceded by % sign
%Before you compile the tex file please download the class file AIMS.cls from the following URL link to the
%local folder where your tex file resides. http://aimsciences.org/journals/tex-sample/AIMS.cls.
\documentclass{aims}
\usepackage{amsmath}
  \usepackage{paralist}
  \usepackage{graphics} %% add this and next lines if pictures should be in esp format
  \usepackage{epsfig} %For pictures: screened artwork should be set up with an 85 or 100 line screen
\usepackage{graphicx}  \usepackage{epstopdf}%This is to transfer .eps figure to .pdf figure; please compile your paper using PDFLeTex or PDFTeXify.
 \usepackage[colorlinks=true]{hyperref}
   % Warning: when you first run your tex file, some errors might occur,
   % please just press enter key to end the compilation process, then it will be fine if you run your tex file again.
   % Note that it is highly recommended by AIMS to use this package.
   \hypersetup{urlcolor=blue, citecolor=red}
%\usepackage{hyperref}

  \textheight=8.2 true in
   \textwidth=5.0 true in
    \topmargin 30pt
     \setcounter{page}{1}

% The next 5 line will be entered by an editorial staff.
\def\currentvolume{X}
 \def\currentissue{X}
  \def\currentyear{200X}
   \def\currentmonth{XX}
    \def\ppages{X--XX}
     \def\DOI{10.3934/xx.xxxxxxx}

 % Please minimize the usage of "newtheorem", "newcommand", and use
 % equation numbers only situation when they provide essential convenience
 % Try to avoid defining your own macros

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}
\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem*{problem}{Problem}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{notation}{Notation}
\newcommand{\ep}{\varepsilon}
\newcommand{\eps}[1]{{#1}_{\varepsilon}}

% This is for Pandoc Markdown -> LaTeX, but AIMS author guidelines do not allow
% custom LATeX macros except for equations!
%\providecommand{\tightlist}{%
%  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
%
%  


%% Place the running title of the paper with 40 letters or less in []
 %% and the full title of the paper in { }.
\title[Teaching Data Science in
Biology] %Use the shortened version of the full title
      {Teaching Data Science to Students in Biology using R, RStudio and
Learnr: Analysis of Three Years Data}

% Place all authors' names in [ ] shown as running head, Leave { } empty
% Please use `and' to connect the last two names if applicable
% Use FirstNameInitial.  MiddleNameInitial. LastName, or only last names of authors if there are too many authors
\author[Guyliann Engels, Philippe Grosjean and Frédérique Artus]{}

% It is required to enter 2020 MSC.
\subjclass{Primary: XXXXX; Secondary: YYYYY.}
% Please provide minimum  5 keywords.
 \keywords{Data Science Teaching, Flipped Classroom, R, RStudio, git.}

% Email address of each of all authors is required.
% You may list email addresses of all other authors, separately.
 \email{Guyliann.Engels@umons.ac.be}
 \email{Philippe.Grosjean@umons.ac.be}
 \email{Frederique.Artus@umons.ac.be}

% Put your short thanks below. For long thanks/acknowledgements,
%please go to the last acknowledgments section.
%\thanks{We would like to acknowledge...}

% Add corresponding author at the footnote of the first page if it is necessary.
% Please add <dollar>^*<dollar> adjacent to the corresponding author's name on the first page.
% The example shown in this template is if the first author is the corresponding author.
\thanks{\textsuperscript{*} Corresponding author: Guyliann Engels}

\begin{document}
\maketitle

% Enter the first author's name and address:
\centerline{\scshape Guyliann Engels\textsuperscript{*}, Philippe Grosjean}
\medskip
{\footnotesize
% please put the address of the first author
 \centerline{Numerical Ecology Department, Complexys and InforTech Institutes, University of Mons}
   %\centerline{Other lines}
   \centerline{Avenue du Champ de Mars, 8, 7000 Mons, Belgium}
} % Do not forget to end the {\footnotesize by the sign }

\medskip

\centerline{\scshape Frederique Artus}

\medskip
{\footnotesize
 % please put the address of the second  and third author
 \centerline{ Pedagogical Support and Quality Assurance Department, University of Mons}
  % \centerline{Other lines}
   \centerline{Place du Parc, 20, 700 Mons, Belgium}
}

\bigskip

% The name of the associate editor will be entered by an editorial staff
% "Communicated by the associate editor name" is not needed for special issue.
% \centerline{(Communicated by the associate editor name)}


%The abstract of your paper
\begin{abstract}
  The courses in biostatistics in biology at the University of Mons,
  Belgium, were completely refactored in 2018 into data science courses
  including computing tools, version management, reproducible analyses,
  critical thinking and open data. Flipped classroom approach is used.
  Students learn with the online material and they apply the concepts on
  individual and group projects using a preconfigured virtual machine
  with R and RStudio. Activities (H5P, learnr or Shiny applications) are
  recorded in a MongoDB database (300,000+ events for 180+ students and
  2,000+ GitHub repositories at several trends. (1) There is a
  relatively long lag period required for the students to get used to
  the computing environment, the teaching method and the data science in
  general. (2) Engagement is very high, with more than 85\% of the
  students that complete all the activities and get good to excellent
  assessment. (3) There is a gap between students' own perception of
  their their progress. (4) During COVID-19 pandemic lockdown, the
  intensity of the activities largely decreased during two weeks before
  returning to previous level, but for 3/4 of the students only. The
  remaining fraction never caught up. We hypothesize that the technical
  requirements or the lack of motivation during the lockdown were
  detrimental to roughly one student over ten, despite all the efforts
  the University deployed to reduce the social fracture. (see
  \url{http://bds.sciviews.org}). The content is expanded beyond
  statistics to \url{https://github.com/BioDataScience-Course}). The
  analysis of these data reveals skills achievements and their
  assessment results: they tend to underestimate
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In a context where there is an exponentially growing mass of data
\cite{Marx2013}, a reproducibility crisis in Science \cite{Baker2016},
and a progressive adoption of Open Science practices \cite{Banks2019},
statistics were broadened to a wider discipline called Data Science. For
the Data Science Association, ``the Data Science means the scientific
study of the creation, validation and transformation of data to create
meaning'' (\url{http://www.datascienceassn.org/code-of-conduct.html}).
These changes also led to the emergence of data science programs in
universities and higher schools \cite{Donoho2017, Cetinkaya-Rundel2021}.
One example is the Harvard Data Science initiative
(\url{https://datascience.harvard.edu/about}) launched in 2017. With a
broader approach, also comes a broaden public. The data science courses
are not just limited to computer scientists, mathematicians or
statisticians, but also welcome students in humanities, social sciences,
and natural sciences (for instance, the data science training at Duke
University \cite{Cetinkaya-Rundel2021}). Focus of such courses is for
students to develop the ability to deal with real datasets in all their
complexities, to be able to conduct reproducible analyses, and to
interpret these data in the light of knowledge in their field of
expertise.

The data transformation part of the job is a challenge for students with
a poor or no background at all in computing. Students that are not used
to deal with computer languages enter in a foreign world and have to
deal with many exotic concepts, techniques and tools. Version control
systems like git, and their internet hosting counterparts like GitHub,
Gitlab or Bitbucket also make part of the tools that a data science
course teaches and use \cite{Fiksel2019, Hsing2019}. Presentation of the
results and the use of document formats that dissociate content from
presentation, namely LaTeX, Jupyter Notebook, or R Markdown to cite a
few, also contribute to the large number of potentially new tools
learners have to discover \cite{Baumer2014}. On the other hand, a
student in computing science already masters one or more computing
languages, is acquainted with version control systems, with databases
and with the way data are manipulated and represented in a computer, but
he may have difficulties to grasp the context of datasets related to
different disciplines. A student in mathematics or statistics is
familiar with various concepts that underpin the techniques used to
analyze the data. Students in biology, medicine, psychology, social
sciences, economics, \ldots{} have obviously very different \emph{a
priori} knowledge. The gap between knowledge and requirements generates
anxiety (see for instance \cite{Onwuegbuzie2003}). The course must be
organized in a way that learners progress by little steps to avoid
exposition to much intimidating concepts and tools at once.

Suitable computer hardware and software environments are required to
apply the concepts in the course. Different approaches range from using
software accessed from a server \cite{Theobold2021} (RStudio Cloud
(\url{https://rstudio.cloud/} \cite{Rstudio2015}), Chromebook data
science (\url{http://jhudatascience.org/chromebookdatascience/})) to
local installation on the student's computers. The former requires
infrastructure to run the software on a server, and that software is
only accessible to the students during the course. The latter raises
problems of license for proprietary software but also installation and
configuration issues. An intermediary solution uses preconfigured
virtual machines, or containers (e.g.~Docker)
\cite{Cetinkaya-Rundel2018, Boettiger2015}. Such a solution is the most
flexible one because it can be deployed almost anywhere (in the computer
lab, at home, on a laptop, \ldots). To apply theoretical concepts
through exercises is a key aspect of learning data science
\cite{Larwin2011}. Correct choice of software is critical and exposing
students early with the tools they are most susceptible to use later in
their work is desirable. This was highlighted by \cite{Auker2020} for
instance, in the case of ecological data.

These data science courses pose several pedagogical challenges because
numerous and unfamiliar concepts must be acquired by a heterogeneous
class population. Learning objectives span a large range of cognitive
abilities and, in these courses, the intended learning outcomes aim at
developing high-level cognitive process abilities such as conceptual,
procedural, and even metacognitive knowledge \cite{Krathwohl2002}. To
meet such learning objectives, active learning methods are useful so
that students could better catch up with these high-level cognitive
skills \cite{Freeman2014}. Teaching and learning frameworks turn to a
blended learning scenario including remote activities to be done before
and after in-class ones, individual and group problem-solving, peer
instructions and ongoing assessment. Indeed, the flipped classroom
approach allows students to be active in their learning, which has the
benefit of improving student outcomes \cite{Freeman2014}. Moreover, it
allows flexibility and enables students to work at their pace. Their
various learning styles are respected as they are actors in their
learning process \cite{Spadafora2018}. Such frameworks are open,
learning centered, and supported by a varied and rich environment
\cite{Burton2011}.

Recently, data science is also used to analyze the effect of different
pedagogical practices on the outcome of these courses thanks to learning
analytics \cite{Estrellado2020}. A vast amount of data can be collected
on students' activities, and the analysis of these data allows comparing
the impact of different pedagogical approaches, or to quantify and
document the impact of changes in the courses \cite{Romero2020}.

At the University of Mons in Belgium, we have started to rework our
biostatistics courses in the biology curriculum in 2018. A series of
Data Science courses were introduced, both for our undergraduate and
graduate students. These courses are inspired from precursor initiatives
cited here above. The goal of these courses is to form biological data
scientists capable of extracting meaningful information from raw
biological data, and to do so in a reproducible way, with the correct
application of statistical tools and an adequate critical mind. A
preconfigured VirtualBox machine with R, RStudio, R Markdown, git, and a
series of R packages preinstalled is used
(\url{https://www.sciviews.org/software/svbox/}) as a very flexible way
to deploy the same software environment both on the university computers
and on student's own laptops.

As our course were reworked, we also decided to use flipped classroom
and progressive adoption of suitable pedagogical practices with a
cyclical approach that consists in stating goals, building pedagogical
material with a large emphasis on numerical tools and collection of
students' activities, and finally, analysis of the data collected. These
results let us regulate our teaching activities the following academic
year with refined goals and improved pedagogical techniques. Here, we
present the main results spanning on three successive academic years
from 2018 to 2021, including two particular periods where distance
learning was forced due to Covid-19 pandemic lockdown. In this paper, we
will focus on the following four questions:

\begin{itemize}
\item
  Transition from theory to practice is critical and tutorials build
  with learnr (\url{https://rstudio.github.io/learnr/}) are capstones in
  our courses. What cognitive workload and perceived workload do these
  tutorials represent for the students?
\item
  Many universities and high-schools resort to exams at the end of an
  academic term. Do such final exams correctly assess the major learning
  outcomes that we expect from our data science courses that is, the
  ability to properly analyse biological data?
\item
  Could we use learning analytics to spot suboptimal learning strategies
  and discriminate different student profiles in our biological data
  science courses?
\item
  Did the quick shift from face-to-face to distance learning imposed by
  Covid-19 lockdown periods affected the production of our students and
  did it required increased exchanges with the teaching staff to support
  it?
\end{itemize}

\hypertarget{methods}{%
\section{Methods}\label{methods}}

The course material is available online (\url{https://wp.sciviews.org})
and is centralized in a Wordpress site. Students have to login with
their GitHub account and their academic data are collected from the
UMONS Moodle server (\url{https://moodle.umons.ac.be}). The courses are
broken down into modules that amount roughly to 15h of work each. There
are two sessions of 2h and 4h in-class (outside lockdown periods, of
course), with roughly 3h of preparation at home before each session, and
3h of work to complete one module. Main activities in the class are
analyzing actual data (projects), answering student questions, and
lecturing briefly (1/4 h) on selected topics. Students propose and vote
for the topics to be covered during these short lectures. Finally, we
encourage students to help each other and to explain what they
understand to their colleagues. Indeed, students' questions may be
redirected by the teacher to other students that have already mastered
the topic. Teachers rarely answer questions directly. When it is
possible, they rather propose new tracks or ideas to investigate and
help learners to find the solution by themselves. Students who go
through the activities before the others are encouraged to help their
colleagues too.

Regarding the timing, one module is taught every second week so that
students have enough time to prepare the material at home before
in-class session, and after it, to finalize their projects. As a term is
made of 14 weeks, we do not teach more than six modules in a course unit
to avoid compacting them to much in time. After reading the theory,
students are exposed to exercises of four increasing levels of
difficulty. They have thus to apply the concepts repeatedly but in
different contexts, which breaks monotony and maintains a stimulating
rhythm all along their progression. Once they have learn the principles
in the book and self-assessed their comprehension of the concepts using
H5P (\url{https://h5p.org}) exercises (level 1 difficulty), they have to
get used to the software environment. Learnr tutorials
(\url{https://rstudio.github.io/learnr/}, level 2) are used to gently
introduce them to the R code required for the analyses by guiding them
through their first data analysis. These tutorials are thus the entry
point to the practice. Projects, first individual (level 3), then in
groups of two to four students (level 4) represent the core activities.
Evaluating these projects constitute, thus, the most important
information to assess the competences of our students.

All students' activities in H5P exercises (self-assessing), and in
learnr tutorials (transitioning smoothly from theory to practice) are
recorded in a MongoDB database. The learnitdown R package
(\url{https://www.sciviews.org/learnitdown/}) provides the code required
to manage user login, user identification and activity tracking for this
interactive material.

Projects containing the data, the analyses and the reports are hosted in
GitHub repositories. These repositories are cloned and edited by the
students in their virtual machines (SciViews Box) with RStudio
(\url{https://www.rstudio.com/products/rstudio/}), either on their
laptops or on the computers in the lab. We encourage our students to
install the virtual machine for the course on their computer so that
they can work comfortably at home and also use it for other activities
too. Assignment and creation of the GitHub repositories for each
student, or group of students, is orchestrated by GitHub Classroom
(\url{https://classroom.github.com}). Reports are written in R Markdown
(\url{https://rmarkdown.rstudio.com/}) that combines the prose with R
code to produce analyses results, plots and tables directly inside the
documents. All repositories are ultimately cloned by the teachers in a
centralized area on our servers and data about commits (git logs) are
collected using git version 2.31.1 and R version 4.0.5
\cite{Rcoreteam2021}. To give an idea of the data recorded, in 2020-2021
we have a little bit more than 3,500 events recorded for each student.

In distance learning, students' support was done via email and Discord
(\url{https://discord.com}). At the end of an academic term, all
recorded messages were collected into text files. These files were
scraped using custom R code to create a table with key information
(basically, who, when, and what) for each message. Surveys are done
periodically in-class though Wooclap questionnaires
(\url{https://www.wooclap.com}). Such a questionnaire was used to query
perceived workload of the learnr tutorials. Wooclap allows to export
data into Excel files. These data are then converted into a table in our
database with an R script.

Data about users, courses, lectures and projects, as well as grading
items (on average, more that 130 grading items were established for each
student in 2020-2021) are anonymized: names, emails and all the personal
information are replaced by random identifiers. The different tables are
ultimately exported into CSV files and made public. These data are
available at {[}\ldots{} Zenodo?{]}. Data collection, treatment, and use
respect European GDPR (General Data Protection Regulation) since each
student had to agree explicitly with the way data are collected and used
(including for research purpose) before each course begins. They can
visualize their data through personalized reports at any time.

The course material is organized in a way that favors autonomy and
self-assessment (direct feedback in the exercises, hints and retry
buttons in case of wrong answers). Table
\ref {tab:tab_ex_levels_summary} summarizes main characteristics of the
exercises according to the difficulty level.

\begin{table}

\caption{\label{tab:tab_ex_levels_summary}\label{tab:tab_ex_levels} Four levels of increasing difficulties in the exercises.}
\centering
\begin{tabular}[t]{l|l|l}
\hline
Level & Description & Type\\
\hline
L1 & Interactive exercise in the course, direct feedback & h5p\\
\hline
L2 & Tutorial with guided exercises, feedback and hints & learnr\\
\hline
L3 & Individual and guided data analysis & individual project\\
\hline
L4 & Free data analysis and reporting (by 2 or 4 students) & group project\\
\hline
\end{tabular}
\end{table}

R and tidyverse packages (\url{https://www.tidyverse.org}) were user to
prepare the data and for the analyses. A GitHub repository with the code
used to create the figures and table in this paper is available at
\url{https://github.com/BioDataScience-Course/teaching_data_science_in_biology}.
The analysis of variance and the post-hoc Honest Significant Difference
(HSD) of Tukey is used for the comparison between courses. The
Self-Organizing Map (SOM) allow an n-dimensional object to be projected
into a 2-dimensional object \cite{Kohonen1995}. Like a principal
component analysis (PCA), this method will bring similar objects closer
together and different objects further apart. The \{kohonen\} R package
implements these methods \cite{Wehrens2018}.

A NASA-LTX questionnaire was used to study perceived workload to
complete a task. It is composed of six questions on a Likert scale
\cite{Hart1988}. The questions concern mental load, physical load, time
pressure, expected success, effort required, and frustration experienced
during the accomplishment of the task. The average value for the six
questions constitutes a Raw Task Load indeX (RTLX) \cite{Byers1989} that
we used to quantify how students feel when using these learnr tutorials.

\hypertarget{results}{%
\section{Results}\label{results}}

This study is performed on data originating from three successive
courses that comprise 26 modules in total in 2020-2021. Table
\ref {tab:tab_course} summarizes the number of H5P, learnr, individual
and group GitHub projects that students had to complete. Group projects
usually span over several modules. It should be noted that for course C,
we also introduced a challenge in machine learning that replaced one
group GitHub project. This challenge is omitted from the present
analysis, being an isolate activity that is difficult to compare to the
rest. However, this explains why there is only one group project in
course C.

\begin{table}

\caption{\label{tab:tab_course_summary}\label{tab:tab_course} Number of students, modules, and exercises for each course. For the learnr tutorials, the first number is the amount of tutorial documents and the second number in brackets is the total number of questions in these tutorials (year 2020-2021).}
\centering
\begin{tabular}[t]{l|r|r|r|l|r|r}
\hline
Course & Students & Modules & H5P & Learnr & Indiv. projects & Group projects\\
\hline
A & 59 & 12 & 59 & 24 (211) & 10 & 4\\
\hline
B & 45 & 8 & 29 & 11 (108) & 12 & 2\\
\hline
C & 26 & 6 & 19 & 7 (37) & 7 & 1\\
\hline
\end{tabular}
\end{table}

Retrospective data from 2018-2019 (only course A) and 2019-2020 (courses
A and B) are also used when it is pertinent. For instance, final exams
were only used during these two years. It should be kept in mind that
the pedagogical material was written and improved progressively during
the three academic years. The H5P exercises and the auto-checking of
learnr answers were not available before 2020-2021. We do not use data
corresponding to our older courses in biostatistics given in a more
traditional way because we consider the comparison is not fair: the
content of these courses is quite different. However, experience
gathered with these old courses during 15 years was critical in the
redesign of the new ones.

\hypertarget{measured-and-perceived-cognitive-workload-in-learnr-tutorials}{%
\subsection{Measured and perceived cognitive workload in learnr
tutorials}\label{measured-and-perceived-cognitive-workload-in-learnr-tutorials}}

In our courses, learnr tutorials play an essential role in the
progressive acquisition of competences because they are at the
transition between the theory (online book chapters) and the practice
(projects). These tutorials are online interactive documents that recall
main concepts, and take the students by the hand to perform their first
data analysis step by step. At each step, they have at least one
exercise or one quiz. The exercise consists in writing R code, or to
fill missing parts in R code to progress in the analysis.

Our goal with these tutorials is to prepare the students optimally for
the practice of data science. On the other hand, we do not want to
exhaust their mental energy just before they start to work on their
projects. The efficiency of these tutorials is qualitatively determined
by observing the behavior of the students when they start their
practical work, but we also have quantitative indicators available, like
the number or retries necessary to complete an exercise on average, the
number of exercises correctly answered, or the time needed to complete
one tutorial.

A few tutorials were elaborated during the academic year 2018-2019, and
positive feedback on their utility (both by direct observation of the
students and thanks to their remarks) led us to systematize them into
what we now call level 2 activities (see Table \ref {tab:tab_course}) in
the form of learnr documents in 2019-2020. The tutorials were further
refined in 2020-2021: we added contextual hints thanks to the gradethis
R package (\url{https://pkgs.rstudio.com/gradethis/}). When students
submit their answer to the exercises, the R code is parsed, analyzed and
the result is compared with the solution. In case of differences,
heuristics are used to provide contextual hints. Students can then
refine their solution and resubmit it. This appears very efficient in
self-teaching and self-assessing their competences before switching to
the practice with confidence.

\begin{figure}
\includegraphics[width=1\linewidth]{teaching_data_science_files/figure-latex/fig_learn_trials-1} \caption{\label{fig:fig_learn_trials} Average number of retries that were required for each student to find the right answer in learnr tutorials exercises (year 2020-2021). This measure is used as an indirect, but objective measurement of the cognitive workload. The black dot is the average for the whole classes and *n* is the number of observations.}\label{fig:fig_learn_trials}
\end{figure}

The objective measurement of the cognitive workload is estimated by
using a proxy: the average number of entries that were required for each
student to find the right answer in learnr tutorial exercises (Fig.
\ref {fig:fig_learn_trials}). Only data from students that performed
most of the exercises were used. This variable varies significantly
between the three courses (ANOVA, F(2,109) = 3.655, p-value = 0.029).
The students in course C need significantly fewer trials to find the
right answer than students in courses A at \(\alpha\) level of 5\%
(Tukey HSD, t = -2.489, p-value = 0.0375) and B (Tukey HSD, t = 0.0474,
p-value = 0.047)

The perceived cognitive load required to perform these exercises is also
determined on the same students and for the same exercises. The Raw Task
Load indeX measures emotional state of the students after having
completed a tutorial. This has, as far as we know, not been studied yet.
We used a NASA LTX questionnaire to assess it across all three courses.
Participation to the survey was high: 48/59 (81\%), 35/45 (78\%) and
18/26 (69\%) for courses A, B, and C respectively.

\begin{figure}
\includegraphics[width=1\linewidth]{teaching_data_science_files/figure-latex/fig_rtlx-1} \caption{\label{fig:fig_rtlx} Perceived workload for the learnr tutorials in the three courses (year 2020-2021). The black circle is the mean RTLX value. The number above each box is the number of respondants.}\label{fig:fig_rtlx}
\end{figure}

The difficulty of the course, and thus, of the exercises in the
tutorials increase from one course to the other. However, we do not
observe any increase, neither in the number of retries, nor in the Raw
Task Load indeX (Fig. \ref {fig:fig_rtlx}). On the contrary, these
appear significantly lower for course C than for course A at the
\(\alpha\) level of 5\% (ANOVA, F(2,98) = 3.588, p-value = 0.031; Tukey
HSD, t = -2.679, p-value = 0.023). The cognitive load perceived by the
students diminishes at the same pace as their ability to find the right
answer in less trials. This may be a consequence of a more fluent R
coding and a better mastering of the software environment.

\hypertarget{final-exam-versus-project}{%
\subsection{\texorpdfstring{Final exam \emph{versus}
project}{Final exam versus project}}\label{final-exam-versus-project}}

An exam at the end of an academic term is a common practice as summative
assessment. So, we compare grades our students got from such an exam
with score they obtain directly in their projects in 2018-2019 and
2019-2020. The final exam is written in learnr, and it mixes a few
questions about the theory with several partly solved data analyses that
students have to explain, criticize and finalize during the exam session
on the computer. The exam is thus largely focusing practical analysis,
in line with the expected outcomes.

\begin{figure}
\includegraphics[width=1\linewidth]{teaching_data_science_files/figure-latex/fig_exams_projects-1} \caption{\label{fig:fig_exams_projects}  Grades obtained at the final exam in function of grades obtained for the projects for courses A and B during two years (course B was still in its old form in 2018-2019 and is thus not represented).}\label{fig:fig_exams_projects}
\end{figure}

The comparison of the grades obtained by each student for a project and
a final exam shows only a weak correlation between these two types of
evaluations (Fig. \ref {fig:fig_exams_projects}). In 2018-2019, only one
student failed in the project, while almost one third of them failed
their final exams. The difficulty of the project was similar to previous
years with the old course in biostatistics (and failure was not uncommon
at that time). The flipped classroom approach allows more time to work
in-class on practical applications, to ask questions, to discuss
results, \ldots{} We hypothesize that the very low failure rate in the
projects could be explained by a better preparation to practical data
analysis, but not to the final exam.

In 2019-2020, we raised a little bit the difficulty for the projects,
resulting in a more widespread distribution of the results, but with a
similar pattern showing very little correlation between the two
evaluation methods. The same conclusion can be drawn for course B, with
several students failing in one of the two evaluations, but not in the
other one.

Despite, a summative assessment that includes a series of questions
involving writing R code to analyze data in the final exam, it does not
reflect properly the ability of the students to correctly process and
analyze biological data, as in their projects. Alignment with intended
learning outcomes seems thus to be more present in the projects. Due to
these results, the final examination was abandoned for the academic year
2020-2021, and it has been replaced by an ongoing assessment of the
students' activities across all four level exercises. These activities
are analyzed in the following section.

\hypertarget{students-activity-profiles-with-ongoing-assessment}{%
\subsection{Students activity profiles with ongoing
assessment}\label{students-activity-profiles-with-ongoing-assessment}}

Despite the fact that we have relatively homogeneous classes of students
with similarly (low) level of knowledge in statistics and computing
initially, the flipped classroom approach and the proactive attitude we
expect from our learners (they must formulate questions correctly
whenever they face a problem) conduct to different and contrasted
learning strategies. Not all students ask questions. Some of them try to
find solutions on their own. Some other prefer to ask their questions
privately, while others have no problems exposing their difficulties on
a public forum (the Discord channel dedicated to the course). The way
and the timing learners progress in the exercises also largely vary. The
schedule is not tight and only suggests a rhythm of progression. No
student is penalized if the exercises are done later, as soon as they
are completed before a final deadline. As expected, a part of our
students prefer to stick to the proposed schedule, while others
procrastinate and differ the completion of their exercises. Some
strategies are more efficient than others. We analyzed records of the
students' activities to distinguish these profiles and we compare them
with the grade they obtain at the end of the course.

In 2020-2021, to support the ongoing assessment without a final exam,
the activity of each student in level 1 (H5P) and 2 (learnr) exercises
was exhaustively recorded in a database. For the GitHub projects (levels
3 and 4 exercises), it is the GitHub repositories and the git log data
that are analyzed. During lockdown periods, exchange with students and
answers to their questions were exclusively done by email, text or voice
messages on Discord on private or public channels. Students were allowed
to freely choose their favorite tool to interact with the teachers and
between each other. All these exchanges were recorded too.

The degree of completion of all the exercises was used to establish the
final grade for the course, with a much higher weight on individuals and
especially, on group projects. The weight was adjusted from course to
course according to the importance of the different projects, mainly. To
give an idea, for course A second term, level 1 H5P exercises accounted
for 5\%, 10\% for level 2 learnr tutorials, 35\% for level 3 individual
projects and 50\% for level 4 group works. On average, each student
received more than 130 assessment items that accounted for their final
grade. Two third of these assessments were established manually, using
evaluation grids and based on their reports in the projects. The
remaining third is made of scores automatically calculated from the
various online exercises.

For the three courses, we recorded more than 450,000 events, which makes
on average almost 3,500 events for each student. These data contain
information to characterize the behavior and learning patterns that the
students use. They are summarized into sixteen metrics.

For H5P exercises:

\begin{itemize}
\item
  trials/H5P ex.: the average number of trials for each H5P exercise
  (students can retry as much as they wish and they have immediate
  feedback if their answer is correct or not),
\item
  correct H5P ex.: the fraction of H5P exercises that were correctly
  answered,
\end{itemize}

For learnr tutorial exercises:

\begin{itemize}
\item
  trials/learnr ex.: the average number of trials for each learnr
  exercise (here also, students can retry as much as they want),
  excluding quizzes,
\item
  hints/learnr ex.: in learnr exercises, students can display hints to
  help them to solve the problems (but they loose 10\% of the score for
  the exercise for each hint they reveal). This is the average number of
  hints per exercise that were displayed by each student,
\item
  correct learnr ex.: the fraction of learnr exercises that were
  completed with a correct answer,
\item
  time/learnr ex.: the average time required to finish one learnr
  exercise involving R code writing, thus excluding quizzes.
\end{itemize}

For individual and group projects:

\begin{itemize}
\item
  commits/ind. projects: the average number of commits done by a student
  in one individual project,
\item
  contributions/ind. projects: the number of lines changed -added or
  subtracted- in the R Markdown reports by one student in one individual
  project, on average (this includes embedded R code for the processing,
  analysis and plotting of data),
\item
  commits/group projects: same as above, but for group projects,
\item
  contributions/group projects: same as above, but for group projects,
\item
  percentage of contributions to group projects: the fraction of work
  the student did, relative to all the work done in group projects.
\end{itemize}

For support:

\begin{itemize}
\item
  questions/module: the number of questions student asked, divided by
  the number of modules in the course,
\item
  percent of public questions: the fraction of questions that the
  student posted in a public channel (the Discord channel dedicated to
  the course that all the other students of the class can read),
\item
  contributions/question: a metric that catches the relative
  ``productivity'' of the student related to the number of questions
  they ask.
\end{itemize}

Finally, global measurements:

\begin{itemize}
\item
  work done: the fraction of all exercises that the student finished,
\item
  work done in time: the fraction the exercises done in the right time,
  that is, during the proposed calendar.
\end{itemize}

In our courses, we have a few students in mobility that come from
various origins. The \emph{a priori} knowledge is important in
education. So, to avoid biases due to the past curriculum of the
students, we restrict this analysis to the subpopulation that comes from
the first year of Bachelor in Biology at UMONS only. A Kohonen'
self-organizing map is used to create student profiles according to
their activities (Fig. \ref {fig:fig_som}). A three by three hexagonal
map was chosen, and students are thus classified into nine different
groups.

\begin{figure}
\includegraphics[width=1\linewidth]{teaching_data_science_files/figure-latex/fig_som-1} \caption{\label{fig:fig_som} Self-organizing map of the student activities across the three courses (year 2020-2021). See the text for the explanations.}\label{fig:fig_som}
\end{figure}

In Fig. \ref {fig:fig_som}, the small peripheral plots in gray scale
show how selected metrics distribute in the nine cells, from lowest
value in white to highest value in black. They help to decipher the way
students behave according to their profile. Metrics that are not
represented in the figure exhibit similar patterns than others (for
instance H5P metrics have a similar pattern as learnr metrics). Dots in
the central plot are the various students, with color representing the
course and the diameter of the dots indicating the grade the students
obtained at the end of the course. The following paragraphs detail
information in that figure. The numbers between brackets mean the cell
number in the central plot, and the upper case letters in brackets refer
to the peripheral subplots.

Although most students finished all, or almost all exercises (D), cell
(3) collects the few students that did only a tiny part of these
exercises. These students obtained very low grades, of course. They
belong to courses A and B. On the other hand, heavy workers are at the
bottom (I \& J), and good performers in learnr tutorials (C) are in
cells (5-9).

\begin{itemize}
\item
  Cells (2 and 6) collect students that seldom ask questions (E), and
  that rarely appear on the public channel (G). Minor differences
  separate them. For instance, learners in cell (2) sometimes use hints
  (B), while those in cell (6) never do, also because they find the
  correct answer to the exercises more often by themselves (C). Asking
  questions is at the core of our pedagogical approach. So, these
  students do not play the game. However, they can possibly succeed.
  Some of them probably exchange with other students through different
  channels that we do not monitor. Cell (2) -more difficulties with
  learnr tutorials- mainly contains students belonging to course, A,
  while cell (6) contains students of courses B and C. There is a clear
  evolution in their behavior from one course to the other in terms of
  ease at realizing the exercises, even if they remain silent in the
  teacher-learner interactions.
\item
  Among the students that have a hard time to figure out the answers to
  auto-evaluation exercises, cell (1) reassemble people that most
  heavily rely on hints (B), and are among those who need to retry the
  exercises more often before figuring out the correct answer (A), a
  characteristic they share with cell (4). These students also ask a lot
  of questions (E), both on the public and private channels (G, mid gray
  indicating a balance between public and private messages). Main
  difference between those two groups is that students in cell (4) try
  harder to find the answer without looking at the hints, while in cell
  (1) they give up more rapidly. Also these students respect the
  proposed schedule much more closely than all others (H). We have
  students coming from all courses there, but a majority from course A.
\item
  All these cells (1-4 plus 6) are students that exhibit suboptimal
  behaviors in one or the other way. The remaining cells (5, 7-9)
  correspond to learner profiles that perform better from this point of
  view. Cell (5) is primarily represented by students from course A, but
  secondly, also from course B and C. These are average actors in all
  metrics, except they are fluent with level 1 (H5P, not shown) and
  level 2 (C) exercises.
\item
  Moving from cell (5) to (7), (8) and (9), we encounter increasingly
  top performers. The number of students from course A becomes
  progressively lower, while course B, and especially C dominate in
  these groups. In cell (7), they largely use the public channel (G) and
  also respect the schedule quite well (H) as main differences from
  those from cell (5). Students in cells (8) and (9) are not so often in
  time, but this is because they are heavier workers in the projects,
  both in the individual (I) and in the group (J) activities. This needs
  obviously more time. In cell (9) we also have the students that
  contribute the most to the reports in terms of lines added or deleted
  (F).
\end{itemize}

To summarize, at the top of the SOM map, cells (1-4, plus 6) contain
students with suboptimal behaviors, cell (5) are average students, and
cells (7-9) at the bottom exhibit profiles corresponding to best
performers. The pattern is also visible between courses A (mainly
distributed at the top or center of the map) to B and C (more
represented at the bottom). This suggests probably that students need
time to get used to the course, its pedagogical approach, and/or the
software environment they have to use. Since only a small fraction of
the participating students fail, excluding the defeating ones in cell
(4), the course pattern can hardly be explained by a filter of the low
performers from one course to the other.

\hypertarget{transition-between-face-to-face-to-distance-learning}{%
\subsection{Transition between face-to-face to distance
learning}\label{transition-between-face-to-face-to-distance-learning}}

Due to Covid-19 lockdown periods, distance learning had to be adopted
abruptly. We analyze the activity collected during academic years
2019-2020 and 2020-2021 to evaluate the impact of these transitions on
the progression of the students In Fig. \ref {fig:fig_support_by_time},
the academic term is divided into seven work periods of approximately
two weeks each (remind that it is the suggested rhythm of the courses:
one module every second week). The classes of the second term start at
period Y1P09, since period Y1P08 is reserved for the exams. The courses
of the first term of 2020-2021 begins at Y2P01. First lockdown started
at period Y1P11 for one month and an half. Second lockdown stared at
Y2P03 and lasted to the end of the second term (Y2P15). During the first
lockdown, we quickly opened the dedicated Discord channels that were
available without any latency.

\begin{figure}
\includegraphics[width=1\linewidth]{teaching_data_science_files/figure-latex/fig_support_by_time-1} \caption{\label{fig:fig_support_by_time} a. Average contributions of the students to the projects by periods of two weeks of course. b. Contributions by question asked (log scale) for each student as a proxy measurement of the efficiency of teacher-learner interactions on the progression in data analyses. Light gray background indicates periods where distance teaching was mandatory due to Covid-19 lockdown. The number of students that interacted during each period is indicated on top of the boxlots (Y1 is 2019-2020, Y2 is 2020-2021).}\label{fig:fig_support_by_time}
\end{figure}

Contributions par student (Fig. \ref {fig:fig_support_by_time}a) is
relatively constant during the second year, starting essentially at
Y2P03, when the second lockdown was established. The highest activity is
observable at the end (Y2P15), although there is no module teach during
that period. This is because of the late students that finalize their
reports at the last minute. Y2P01, Y2P02 and Y2P09 exhibit the lowest
activity, and these are the start of the first and second terms. Y2P01
and Y2P01 were also teach in face-to-face and they correspond to the
start of all three courses.

The efficiency of learners-teachers interactions for that contribution,
quantified by the contributions divided by the questions (Fig.
\ref {fig:fig_support_by_time}b) is very widespread from one student to
the other. That ratio spreads on several orders of magnitude. However,
median value -the bar inside the boxes- varies much less. Global amount
of questions during each period is less variable, as is the absolute
contributions, leading to a rather stable ratio. Highest median ratios
are observed at the last period of each term (Y2P07 and Y2P15) although
no module was teach at that time. More contributions are observed
relative to the questions at the end: students essentially finalize
their reports.

The first year shows a different pattern. First, the lockdown period was
restricted to the very end of second term. Only the last module in both
course A and B remained. In Y1P12, when distance learning was first
imposed, we observe a marked decrease in the contributions per student
(Fig. \ref {fig:fig_support_by_time}a). It is heavily compensated in
periods Y1P13 and Y1P14, which are by far the most busy periods of all.
Period Y1P15 is not represented because it is after the deadline to
finish all work that year.

Efficiency of support during the first year shows a similar pattern as
for second year: extremely widespread from student to student. Median
value is similar too, if not among the highest during periods Y1P11,
Y1P13 and Y1P14. The productivity was thus not affected during that
first lockdown, after a short lag time observable in Y1P12.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Teaching data science to a population of students that are not very used
to advanced computer techniques and tools, and that have only basic
knowledge in mathematics and statistics is a hard task \cite{Sousa2018}.
Our basic approach is to extend the training on a very long period: five
successive terms spanning on three consecutive years (undergraduate and
graduate). That way, the many different concepts they have to learn can
be broken down into subunits (26 modules) that last for two weeks each.
We also used highly developed flipped classrooms and blended teaching
and learning (following Spadafora \& Zopito's definition of ``any
educational model where online delivery ranges from 50\% to 80\%''
\cite{Spadafora2018}), with an emphasis on proactive exchange with the
teachers: students have to ask questions to progress. Overall, these are
winning choices because most of our students are successful, excluding a
few defeating ones. \cite{Compeau2019} also obtained good results using
flipped classroom with its course targeting students in biology.

Despite our courses, the students are more used to a traditional
face-to-face approach made of lectures followed by exercises where
important concepts are repeated at the beginning of the practical
sessions. They tend to have a passive attitude during lectures and they
expect the teachers and assistants feeding them with the key concepts.
That attitude does not purposely work here. Proactive behavior and
developing autonomy are required \cite{Freeman2014}. They thus have to
engage themselves in a very different way of learning. The transition
between the theory they read in the book and the projects where they
have to apply these concepts is too abrupt without a progression that
facilitiates students' ebgagement, in four stages, which are: (1)
auto-evaluation exercises directly in the online book, (2) recall of the
main concepts and guided step-by-step analysis of a first dataset with
the learnr tutorials and (3) at least one guided individual project with
another dataset, before (4) they are presented yet another dataset, with
limited instructions this time. Task two, the learnr tutorial, was
immediately spotted as a key activity in the learning process during
academic year 2018-2019. So, we have focused our attention on these
learnr documents. In 2020-2021, the use of an heuristic engine
(gradethis) to provide contextual feedback on the errors students make
in their answers was much appreciated. The measured RTLX index will
serve in the future as a reference to gauge possible optimization of the
tutorials, with lower perceived workload without sacrificing the
content. The significant decrease in RTLX value from course A to C
indicates that there is still a margin of progression. We would like to
observe such a decrease sooner, perhaps already in the second course. It
is indeed important ``to maintain reflective and systematic approaches
in both the development and evaluation {[}of our{]} blended approach''
\cite{Spadafora2018}.

Ultimately, the task that better evaluates their practical skills in
biological data analysis, and which meets with the courses' outcomes, is
the group project because students have to demonstrate what they can do
in situation with a complex dataset, and minimal instructions. They have
to figure out a suitable question, analyze the data to answer that
question, present their results in a report and discuss what they found
with a critical mind \cite{Auker2020}. This task is complex, especially
for undergraduate students. That is why we organize it in groups of two
to four students, depending on the complexity of the problem. Here, they
mobilize their collective intelligence to get results as many of them
would be hardly capable of achieving the task alone. The tracking of
individual activity in the project thanks to git allows figuring out
clearly what was the contribution of each student in the group and to
score their individual contributions suitably.

Sometimes, groups do not work well, and one student has to do most of
the work. This is a clear weakness in this approach, especially if one
of the defeating students in Fig. \ref {fig:fig_som} cell (3) is
involved. If we could identify the profile of the different students
relatively early during the course, we would be able to create better
groupings with a blend of different complimentary profiles to enrich the
experience of all learners. Maybe should we work exclusively with groups
of four to mitigate the impact of one defeating student? The balance of
heterogeneous competences and complementarity are essential in a groupin
order to create mutual emulation and efficacy {[}TODO: add this
reference:MUCCHIELLI R., Le travail en équipe, ESF Editions, Paris,
1996{]}. Working on group composition will thus be one of our future
challenges.

We observed a low correlation between performances in group projects and
in grades obtained at final exams (Fig. \ref {fig:fig_exams_projects}).
This led us to stop using final exams, at the benefit of an ongoing
assessment with emphasis into projects. We had to set up a procedure to
monitor and score the students' activity in all the exercises by
automatic scoring online exercises, and to manually score all projects
against evaluation grids. This is time-consuming despite the partially
automatic scoring lowers the charge. It would be interesting to
investigate whether tools derived from learning analytics and computing
science could second teachers in this work. For instance, reproducible
research is one of the competences our students have to develop. It
implies that their R Markdown documents should compile into reports in
HTML or PDF format without any error. This criterion could be checked
automatically.

Activity tracking in the exercises, primarily set up for the continuous
evaluation, also offers the opportunity to study the way learning
happens (or not). Indeed, Learning analytics provides opportunities to
monitor learning events but also to adjust teaching to improve student
learning {[}TODO: add these references: Martin \& Ndoye, 2016 ; Romero
\& Ventura 2019{]}, even if they are primarily used to early predict
success or failure. In our courses, failure rate is already rather low
and essentially limited to a few defeating students that do not work at
all. We are more interested to classify our participating students
according to their behaviors. This paves the way toward a more inclusive
pedagogy by spotting different kinds of suboptimal patterns (for
instance, never asking questions, looking at hints too quickly without
really trying to figure out the answer, being shy to discuss problems on
public channels, \ldots) Once these patterns are evidenced, we can think
of countermeasures. As Martin \& Ndoye highlight from others studies,
``benefits that the online learning platform provides with respect to
assessment include better monitoring opportunities for student learning
and immediate feedback {[}\ldots{]}, and individual practice
opportunities'' {[}TODO: add reference: Martin \& Ndoye, 2016{]}. As an
example for students that rarely post their questions publicly, we will
test an alternate discussion channel were teachers never post but have
read access. In case of an error, the teacher will contact the student
privately to explain him what is wrong. That student would then have the
responsibility to reexplain corretly to its siblings. This way, its
error is never publicly spotted by the teacher. This approach was very
successful with our so-called ``eleve-assitants'' tutor-students from
higher classes that have brilliantly succeeded in the course one or two
years ago and that second the teaching staff-. With tools like the
self-organizing map we should be able to predict suboptimal student
profiles early. We could engage a discussion with the concerned persons
to determine the cause and find a solution as early as possible.
Learning analytics used that way would promote a differential
pedagogical approach, a key for more inclusive teaching
\cite{Siemens2013}.

Forced distance learning, due to Covid-19 lockdown did not appear to be
a barrier in the production of our students in their projects. A pattern
is observed during first lockdown with a marked decrease in their
contributions, followed by a large, compensatory activity. All this
happened in a time frame of a couple of weeks. That was the time needed
to adapt to the new situation. The most problematic aspect was the
access to a powerful-enough computer for roughly 15 to 20\% of our
students. In a normal situation, these students had access to computers
at the university, both in-class and outside of class time. When
lockdown was imposed, those students suffered most by a lack of
hardware. However, to reduce the social fracture, the university quickly
reacted and computers were lent to them. During the second lockdown, a
larger part of our students had acquired their own computer, and
solutions were immediately available for the others. Consequently, no
lag time was observed in their production.

If the amount of questions and the efficiency of the teacher-learner
interactions, quantified in terms of contributions/question, remained
globally at a similar level in face-to-face or distance learning, but
their impact on the teachers timetables was very different. In distance
learning, students tend to work at very different moments. Their
questions are thus less concentrated during the course periods. Also, an
alternation between asynchronous work at home and synchronous work in
the computer lab is more beneficial to interactions between students.
The social and human components of teaching and learning are key factors
that tend to vanish in exclusive distance learning. Contacts through
videoconferences only partly compensate lack of interactions because
in-class presence remains different to video chats. Blended learning
combines the best of the two practices if pedagogical setups are
accurate and well-balanced {[}TODO: add this reference: Bernard,
2014{]}.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Teaching data science comes with challenges. The discipline is quite
young, and we still are seeking the best pedagogical approach. After
three years of teaching data science to undergraduate and graduate
students in a curriculum in biology with revised pedagogical practices,
we have our first cohort that has passed all three courses. There are
still two optional courses available in second year of the Master if
they want to push their data science skills further on. However, the
three mandatory courses are designed to be sufficient by themselves.
Globally, most students acquired the competences during these courses.
We have the feeling that they are more mature and more capable in data
science by effectively acquiring the intended outcomes, than with our
previous courses in biostatistics given in a more traditional way. The
impact of the revised approach to teach biological data science on the
way learners manage data and data analysis will be observable during the
following years. We will monitor how these students apply their skills
in their Master thesis, and later, in their career or during their PhD
thesis. Meanwhile, we will continue to improve our courses by further
exploiting the data we accumulate on the activity of our students.
Experience gathered during forced distance learning during Covid-19
lockdown will be used too to improve our courses. The radical changes
that were required in that context showed that students can accommodate
to a large extent, but also that a diversification of the activities is
beneficial to guarantee their engagement \cite{Spadafora2018} {[}TODO:
add reference: Young 2002 in Lloyd-Smidth 2010{]}. Speaking about
diversification, in 2020-2021 we have successfully tested a kaggle-like
challenge (\url{https://www.kaggle.com/competitions}) in one of the
machine learning modules. Such playful activities would also contribute
to the diversification of pedagogical practices, interest and motivation
of the students \cite{Alonso2019}. We would also be happy to share
experience with other teachers in data science. Altogether, we are on
the way to reshape the post-covid teaching landscape, and it will
probably be quite different to what we are used today!

% You may incorporate your references as follows in your main tex file.
% Using BibTex is not recommended but can be handled.

\begin{thebibliography}{99}
%bibliography.bib

\bibitem{Alonso2019} [10.1016/j.compedu.2019.103612]
     \newblock  C. Alonso-Fernandez,  A. Calvo-Morata, M. Freire, I. Martínez-Ortiz and B. Fernandez-Manjon,
     \newblock Applications of data science to game learning analytics data: A systematic literature review,
     \newblock \emph{Computers \& Education}, \textbf{141} (2019), 103612.

\bibitem{Auker2020} [10.1002/ecs2.3060]
     \newblock  L. A. Auker and E. L. Barthelmess,
     \newblock Teaching R in the undergraduate ecology classroom: approaches, lessons learned, and recommendations,
     \newblock \emph{Ecosphere}, \textbf{11} (2020), e03060.

\bibitem{Baker2016} [10.1038/533452a]
     \newblock  M. Baker,
     \newblock 1,500 scientists lift the lid on reproducibility,
     \newblock \emph{Nature}, \textbf{533} (2016), 452--454.

\bibitem{Banks2019} [10.1007/s10869-018-9547-8]
     \newblock  G. C. Banks, J. G. Field, F. L. Oswald, E. H. O'Boyle, R. S. Landis, D. E. Rupp, S. G. Rogelberg.
     \newblock Answers to 18 Questions About Open Science Practices,
     \newblock \emph{Journal of Business and Psychology}, \textbf{34} (2019), 257--270.

\bibitem{Baumer2014} [10.5070/t581020118]
     \newblock B. Baumer, M. Cetinkaya-Rundel, A. Bray, L. Loi and N. J. Horton,
     \newblock R Markdown: Integrating A Reproducible Analysis Tool into Introductory Statistics,
     \newblock \emph{Technology Innovations in Statistics Education}, \textbf{8} (2014).

\bibitem{Bernard2014} [10.1007/s12528-013-9077-3]
     \newblock  L. Lloyd-Smith,
     \newblock A meta-analysis of blended learning and technology use in higher education: from the general to the applied,
     \newblock \emph{J Comput High Educ}, \textbf{26} (2014), 87–-122.

\bibitem{Boettiger2015} [10.1145/2723872.2723882]
     \newblock  C. Boettiger,
     \newblock An Introduction to Docker for Reproducible Research,
     \newblock \emph{SIGOPS Oper. Syst. Rev.}, \textbf{49} (2015), 71--79.

\bibitem{Burton2011}
     \newblock  R. Burton, S. Borruat, B. Charlier, N. Coltice, N. Deschryver, F. Docq and al.,
     \newblock Vers une typologie des dispositifs hybrides de formation en enseignement supérieur,
     \newblock \emph{Distances et savoirs}, \textbf{9} (2011), 69--96.

\bibitem{Byers1989}
     \newblock  J.C. Byers, A. Bittner and S. Hill,
     \newblock Traditional and raw task load index (TLX) correlations: Are paired comparisons necessary?,
     \newblock \emph{Advances in Industrial Ergonomics and Safety}, 1989, 481--485.

\bibitem{Cetinkaya-Rundel2018} [10.1080/00031305.2017.1397549]
     \newblock  M. Cetinkaya-Rundel and C. Rundel,
     \newblock Infrastructure and Tools for Teaching Computing Throughout the Statistical Curriculum,
     \newblock \emph{American Statistician}, \textbf{72} (2018), 58--65.

\bibitem{Cetinkaya-Rundel2021} [10.1080/10691898.2020.1804497]
     \newblock  M. Cetinkaya-Rundel and V. Ellison,
     \newblock A Fresh Look at Introductory Data Science,
     \newblock \emph{Journal of Statistics Education}, \textbf{0} (2021), 1--27.

\bibitem{Compeau2019} [10.1080/10691898.2020.1804497]
     \newblock P. Compeau,
     \newblock Establishing a computational biology flipped classroom,
     \newblock \emph{PLoS Computational Biology}, \textbf{15} (2019), 1--8.

\bibitem{Donoho2017} [10.1080/10618600.2017.1384734]
     \newblock  D. Donoho,
     \newblock 50 Years of Data Science,
     \newblock \emph{Journal of Computational and Graphical Statistics}, \textbf{26} (2017), 745--766.

\bibitem{Estrellado2020} [10.1007/978-1-4612-0873-0]
     \newblock R.A. Estrellado, E. A. Emily, J. Mostipak, J. M. Rosenberg and I. C. Velasquez,
     \newblock \emph{Data science in education using R},
     \newblock 1st edition, Routledge, London, England, 2020.

\bibitem{Fiksel2019} [10.1080/10691898.2019.1617089]
     \newblock J. Fiksel, L. R. Jager, J. S. Hardin and M. A. Taub
     \newblock Using GitHub Classroom to teach statistics,
     \newblock \emph{Journal of Statistics Education}, \textbf{27} (2019), 110--119.

\bibitem{Freeman2014} [10.1073/PNAS.1319030111]
     \newblock  D. R. Krathwohl,
     \newblock Active learning increases student performance in science, engineering, and mathematics,
     \newblock \emph{Proceedings of the National Academy of Sciences}, \textbf{111} (2014), 8410--8415.

\bibitem{Hart1988}
     \newblock S. G. Hart and L. E. Staveland,
     \newblock Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research,
     \newblock \emph{Advances in Psychology}, \textbf{52} (1998), 139--183.

\bibitem{Hsing2019} [10.1145/3287324.3287460]
     \newblock C. Hsing and V. Gennarelli,
     \newblock Using GitHub in the Classroom Predicts Student Learning Outcomes and Classroom Experiences: Findings from a Survey of Students and Teachers,
     \newblock In Proceedings of \emph{the 50th ACM Technical Symposium on Computer Science Education (SIGCSE '19)}, 2019, 672–678.

\bibitem{Kohonen1995} [10.1007/978-3-642-97610-0]
     \newblock T.  Kohonen,
     \newblock \emph{Self-Organizing Maps},
     \newblock 1st edition, Springer-Verlag, Berlin Heidelberg, 1995.

\bibitem{Krathwohl2002} [10.1207/s15430421tip4104]
     \newblock  D. R. Krathwohl,
     \newblock A Revision of Bloom' s Taxonomy: An overview,
     \newblock \emph{Theory Into Practice}, \textbf{41} (2002), 212--218.

\bibitem{Larwin2011} [10.1080/15391523.2011.10782572]
     \newblock  K. Larwin and D. Larwin,
     \newblock A meta-analysis examining the impact of computer-assisted instruction on postsecondary statistics education: 40 years of research,
     \newblock \emph{Journal of Research on Technology in Education}, \textbf{43} (2011), 253--278.

\bibitem{Lloyd-smith2010}
     \newblock  L. Lloyd-Smith,
     \newblock Exploring the Advantages of Blended Instruction at Community Colleges and Technical Schools,
     \newblock \emph{Journal of Online Learning and Teaching}, \textbf{6} (2010).

\bibitem{Martin2016}
     \newblock  F. Martin and A. Ndoye,
     \newblock Using Learning Analytics to Assess Student Learning in Online Courses,
     \newblock \emph{Journal of University Teaching \& Learning Practice}, \textbf{13} (2016), 69--96.

\bibitem{Marx2013} [10.1038/498255a]
     \newblock  V. Marx,
     \newblock The big challenges of big data,
     \newblock \emph{Nature}, \textbf{498} (2013), 255--260.

\bibitem{Onwuegbuzie2003} [10.1080/1356251032000052447]
     \newblock J. A. Onwuegbuzie  and V. A. Wilson,
     \newblock Statistics Anxiety: Nature, etiology, antecedents, effects, and treatments--A comprehensive review of the literature,
     \newblock \emph{Teaching in Higher Education}, \textbf{8} (2003), 195--209.

\bibitem{Romero2020} [10.1002/widm.1355]
     \newblock C. Romero and S. Vetura,
     \newblock Educational data mining and learning analytics: An updated survey,
     \newblock \emph{WIREs Data Mining Knowl Discov}, \textbf{10} (2020), e1355.


\bibitem{Rcoreteam2021}
     \newblock R Core Team (2021),
     \newblock R: A Language and Environment for Statistical Computing,
     \newblock R Foundation for
  Statistical Computing, Vienna, Austria. \url{
  https://www.R-project.org}

\bibitem{Rstudio2015}
     \newblock RStudio team (2015),
     \newblock RStudio Cloud,
     \newblock Boston, MA: RStudio, Inc.

\bibitem{Siemens2013} [10.1177/0002764213498851]
     \newblock G. Siemens,
     \newblock Learning Analytics: The Emergence of a Discipline,
     \newblock \emph{American Behavioral Scientist}, \textbf{57} (2013), 1380--1400.

\bibitem{Sousa2018} [10.1145/3287324.3287460]
     \newblock B. Sousa and D. Gomes,
     \newblock Teaching With R—-A Curse or a Blessing?,
     \newblock In Proceedings of \emph{the Tenth International Conference on Teaching Statistics (ICOTS10 '18)}, 2018, 672–678.

\bibitem{Spadafora2018} [10.5206/cjsotl-rcacea.2018.1.6]
     \newblock N. Spadafora and Z. Marini
     \newblock Self-Regulation and “Time Off”: Evaluations and Reflections on the Development of a Blended Course,
     \newblock \emph{The Canadian Journal for the Scholarship of Teaching and Learning}, \textbf{9} (2018).

\bibitem{Theobold2021} [10.1080/10691898.2020.1854636]
     \newblock A. S. Theobold, A. Hancock and S. Mannheimer,
     \newblock  Designing Data Science Workshops for Data-Intensive Environmental Science Research,
     \newblock \emph{Journal of Statistics and Data Science Education}, \textbf{29} (2021), S83--S94.

\bibitem{Wehrens2018} [10.18637/jss.v087.i07]
     \newblock R. Wehrens and J. Kruisselbrink,
     \newblock  Flexible Self-Organizing Maps in kohonen 3.0,
     \newblock \emph{Journal of Statistical Software, Articles}, \textbf{87} (2018), 1--18.

\end{thebibliography}

\medskip
% The data information below will be filled by AIMS editorial staff
Received xxxx 20xx; revised xxxx 20xx.
\medskip

\end{document}
