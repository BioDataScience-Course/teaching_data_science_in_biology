---
title: 'Teaching Data Science to Students in Biology using R, RStudio and Learnr:
  Analysis of Three Years Data'
runtitle: Teaching Data Science in Biology
abstract: |
    The courses in biostatistics in biology at the University of Mons, Belgium,
    were completely refactored in 2018 into data science courses
    include computing tools, version management, reproducible analyses, critical
    thinking and open data. Flipped classroom approach is used. Students learn
    with the online material and they apply the concepts on individual and group
    projects using a preconfigured virtual machine with R and RStudio. Activities
    (H5P, learnr or Shiny applications) are recorded in a MongoDB database
    (300,000+ events for 180+ students and 2,000+ GitHub repositories at
    several trends. (1) There is a relatively long lag period required for the
    students to get used to the computing environment, the teaching method and
    the data science in general. (2) Implication is very high, with more than 85%
    of the students that complete all the activities and get good to excellent
    assessment. (3) There is a gap between students' own perception of their
    their progress. (4) During COVID-19 pandemic lockdown, the intensity of the
    activities largely decreased during two weeks before returning to previous
    level, but for 3/4 of the students only. The remaining fraction never caught
    up. We hypothesize that the technical requirements or the lack of motivation
    during the lockdown were detrimental to roughly one student over ten, despite
    all the efforts the University deployed to reduce the social fracture.
    (see http://bds.sciviews.org). The content is expanded beyond statistics to
    https://github.com/BioDataScience-Course). The analysis of these data reveals
    skills achievements and their assessment results: they tend to underestimate
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    latex_engine: xelatex
    keep_tex: yes
    template: template.tex
  word_document: default
  html_document: 
    code_folding: hide
    fig_caption: yes
#bibliography: bibliography.bib
#csl: aims-mathematics.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
SciViews::R
```

# Introduction

In a context where there is an exponentially growing mass of data \cite{Marx2013}, a reproducibility crisis in Science \cite{Baker2016}, and a progressive adoption of Open Science practices \cite{Banks2019}, statistics were broadened to a wider discipline called Data Science. For the Data Science Association, "the Data Science means the scientific study of the creation, validation and transformation of data to create meaning" (<http://www.datascienceassn.org/code-of-conduct.html>). These changes also led to the emergence of data science programs in universities and higher schools \cite{Donoho2017, Cetinkaya-Rundel2021}. One example is the Harvard Data Science initiative (<https://datascience.harvard.edu/about>) started in 2017. With a broader approach, also comes a broaden public. The data science courses are not just limited to computer scientists, mathematicians or statisticians, but also welcome students in humanities, social sciences, and natural sciences (for instance, the data science training at Duke University \cite{Cetinkaya-Rundel2021}). Focus of such courses is for students to develop the ability to deal with real datasets in all their complexities, to be able to conduct reproducible analyses, and to interpret these data in the light of knowledge in their field of expertise.

The data transformation part of the job is a challenge for students with a poor or no background at all in computing. Students that are not used to deal with computer languages enter in a foreign world and have to deal with many exotic concepts, techniques and tools. Version control systems like git, and their internet hosting counterparts like GitHub, Gitlab or Bitbucket also make part of the tools that data science course teaches and use \cite{Fiksel2019, Hsing2019}. Presentation of the results and the use of document formats that dissociate content from presentation, namely LaTeX, Jupyter Notebook, or R Markdown to cite a few, also contribute to the large number of potentially new tools learners have to discover \cite{Baumer2014}. On the other hand, a student in computing science already masters one or more computing languages, is acquainted with version control systems, with databases and with the way data are manipulated and represented in a computer, but he may have difficulties to grasp the context of datasets related to different disciplines. Likewise, a student in mathematics or statistics is familiar with various concepts that underpins the techniques used to analyze the data. Students in biology, medicine, psychology, social sciences, economics, ... have obviously very different *a priori* knowledge. The gap between knowledge and requirements generates anxiety (see for instance \cite{Onwuegbuzie2003}). The course must be organized in a way that learners progress by little steps to avoid exposition to much intimidating concepts and tools at once.

Suitable computer hardware and software environments are required to apply the concepts in the course. Different approaches range from using software accessed from a server (RStudio Cloud (<https://rstudio.cloud/> [TODO: add citation here]), Chromebook data science (<http://jhudatascience.org/chromebookdatascience/>) [TODO: add citation here]) to local installation on the student's computers. The former requires infrastructure to run the software on a server, and that software is only accessible to the students during the course. The later raises problems of license for proprietary software but also installation and configuration issues. An intermediary solution uses preconfigured virtual machines, or containers (e.g. Docker) \cite{Cetinkaya-Rundel2018, Boettiger2015}. Such a solution is the most flexible one because it can be deployed almost anywhere (in the computer lab, at home, on a laptop, ...). To apply theoretical concepts through exercises is a key aspect of learning data science \cite{Larwin2011}. Correct choice of software is critical and exposing students early with the tools they are most susceptible to use later in their work is desirable. This was highlighted by \cite{Auker2020} for instance, in the case of ecological data.

These data science courses pose several pedagogical challenges due to the numerous and unfamiliar concepts that must be acquired by a heterogeneous class population. Learning objectives span a large range of cognitive abilities and, in these courses, the intended learning outcomes aim at developing high-level cognitive process abilities such as conceptual, procedural, and even metacognitive knowledge \cite{Krathwohl2002}. To meet such learning objectives, active learning methods are useful so that students could better catch up with these high-level cognitive skills \cite{Freeman2014}. Teaching and learning frameworks turn to a scenario including remote activities to be done before in-class ones, individual and group problem-solving, peer instructions and ongoing assessment. Indeed, the flipped classroom approach allows students to be active in their learning, which has the benefit of improving student outcomes \cite{Freeman2014}. Moreover, it allows flexibility and enables students to work at their pace. Their various learning styles are respected as they are actors in their learning process [Spadafora & Zopito, 2018: TODO ref à ajouter]. As it is described in the Methods part, the framework is open, learning centered, and supported by a varied and rich environment [TODO: cite Bruton et al., 2011].

Recently, data science is also used to analyze the effect of different pedagogical practices on the outcome of these courses thanks to learning analytics \cite{Estrellado2020}. A vast amount of data can be collected on students’ activities, and the analysis of these data allows comparing the impact of different pedagogical approaches, or to quantify and document the impact of changes in the courses. [TODO: detail a little bit with one more reference]

At the University of Mons in Belgium, we have started to rework our biostatistics courses in the biology curriculum in 2018. A series of Data Science courses were introduced, both for our undergraduate and graduate students. These courses are inspired from precursor initiatives cited here above. The goal of these courses is to form biological data scientists capable of extracting meaningful information from raw biological data, and to do so in a reproducible way, with the correct application of statistical tools and an adequate critical mind. A preconfigured VirtualBox machine with R, RStudio, Rmarkdown, git, and a series of R packages preinstalled is used (<https://www.sciviews.org/software/svbox/>) as a very flexible way to deploy the same software environment both on the university computers and on student's own laptops.

As our course were reworked, we also decided to use flipped classroom and progressive adoption of suitable pedagogical practices with a cyclical approach that consists in stating goals, building pedagogical material with a large emphasis on numerical tools and collection of students' activities, and finally, analysis of the data collected. These results let us regulate our teaching activities the following academic year with refined goals and improved pedagogical techniques. Here, we present the main results spanning on three successive academic years from 2018 to 2021, including two particular periods where distance learning was forced due to Covid-19 pandemic lockdown. In this paper, we will focus on the following four questions:

- Transition from theory to practice is critical and tutorials build with learnr (<https://rstudio.github.io/learnr/>) are capstones in our courses. What cognitive workload and perceived workload do these tutorials represent for the students?

- Many universities and high-schools resort to exams at the end of an academic term. Do such final exams correctly assess the major learning outcomes that we expect from our data science courses that is, the ability to properly analyse biological data?

- Could we use learning analytics to spot suboptimal learning strategies and discriminate different student profiles in our biological data science courses?

- Did the quick shift from face-to-face to distance learning imposed by Covid-19 lockdown periods affected the production of our students and did it required increased support from the teaching staff to support it?


# Methods

The course materials are available online (https://wp.sciviews.org) and are centralized in a Wordpress site. Students have to login with their GitHub account and their academic data are collected from the UMONS Moodle server (<https://moodle.umons.ac.be>). The courses are broken down into modules that amount roughly to 15h of work each. There are two sessions of 2h and 4h in-class (outside lockdown periods, of course), with roughly 3h of preparation at home before each session, and 3h of work to complete one module. Main activities in the class are analyzing actual data (projects), answering student questions, and lecturing briefly (1/4 h) on selected topics. Students propose and vote for the topics to be covered during these short lectures. Finally, we encourage students to help each other and to explain what they understand to their colleagues. Indeed, students' questions may be redirected by the teachers to other students that have already mastered the topic. Teachers rarely answer questions directly. When it is possible, they rather propose new tracks or ideas to investigate and help learners to find the solution by themselves. Students who go through the activities before the others are encouraged to help their colleagues too.

Regarding the timing, one module is taught every second week so that students have enough time to prepare the material at home before in-class session, and after it, to finalize their projects. As a term is made of 14 weeks, we do not teach more than six modules in a course unit to avoid compacting them in time at a faster pace. After reading the theory, students are exposed to exercises of four increasing levels of difficulty. They have thus to apply the concepts repeatedly but in different contexts, which breaks monotony and maintains a stimulating rhythm all along their progression. Once they have learn the principles in the book and self-assessed their comprehension of the concepts using H5P (<https://h5p.org>) exercises (level 1 difficulty), they have to get used to the software environment. Learnr tutorials (<https://rstudio.github.io/learnr/>, level 2) are used to gently introduce them to the R code required for the analyses by guiding them through their first data analysis. These tutorials are thus the entry point to the practice. Projects, first individual (level 3), then in groups of two to four students (level 4) represent the core activities. Evaluating these projects constitute, thus, the most important information to assess the competences of our students. 

All students' activities in H5P exercises (self-assessing), and in learnr tutorials (transitioning smoothly from theory to practice) are recorded in a MongoDB database. The learnitdown R package (<https://www.sciviews.org/learnitdown/>) provides the code required to manage user login, user identification and activity tracking for this interactive materials.

Projects containing the data, the analyses and the reports hosted in GitHub repositories. These repositories are cloned and edited by the students in the virtual machines (SciViews Box) with RStudio (<https://www.rstudio.com/products/rstudio/>), either on their laptops or on the computers in the lab. We encourage our students to install the virtual machine for the course on their computer so that they can work comfortably at home and also use it for other activities too. Assignment and creation of the GitHub repositories for each student, or group of students is orchestrated with GitHub Classroom (<https://classroom.github.com>). All repositories are ultimately cloned in a centralized area on our servers and data about commits (git logs) are collected using git version 2.31.1 and R version 4.0.5 [TODO: cite R properly here]. To give an idea of the data recorded, in 2020-2021 we have a little bit more than 3,500 events recorded for each student.

In distance learning, students' support was done via email and Discord (<https://discord.com>). At the end of an academic term, all recorded messages are collected into text files. These files are scraped using custom R code to create a table with key information (basically, who, when, and what) for each message. Surveys are done periodically in-class though Wooclap questionnaires (<https://www.wooclap.com>). Such a questionnaire was used to query perceived workload of the learnr tutorials. Wooclap allows to export data into Excel files. These data are then converted into a table in our database with an R script.

Data about users, courses, lectures and projects, as well as grading items (on average, more that 130 grading items were established for each student in 2020-2021) are anonymized: names, emails and all the personal information are replaced by random identifiers. The different tables are ultimately exported into CSV files and made public. These data are available at [... Zenodo?]. Data collection, treatment, and use respect European GDPR (General Data Protection Regulation) since each student had to agree explicitly with the way data are collected and used (including for research purpose) before each course begins. They can visualize their data through personalized reports at any time.

The course material is organized in a way that favors autonomy and self-assessment (direct feedback in the exercises, hints and retry buttons in case of wrong answers). Table \ref {tab:tab_ex_levels_summary} summarizes main characteristics of the exercises according to the difficulty level, for one to four.

```{r tab_ex_levels_summary}
tibble::tribble(
  ~Level, ~Description, ~Type,
  "L1", "Interactive exercise in the course, direct feedback", "h5p",
  "L2", "Tutorial with guided exercises, feedback and hints", "learnr",
  "L3", "Individual and guided data analysis", "individual project",
  "L4", "Free data analysis and reporting (by 2 or 4 students)", "group project"
) %>.%
  knitr::kable(., format = "latex", caption = "\\label{tab:tab_ex_levels} Four levels of increasing difficulties in the exercises.")
```

R and tidyverse packages (<https://www.tidyverse.org>) were user to prepare the data and for the analyses. A GitHub repository with the code used to create the figures and table in this paper is available at <https://github.com/BioDataScience-Course/teaching_data_science_in_biology>. [TODO: more details on statistic analyses, in particular SOM to put here + refs... Guyliann?]

The NASA-LTX questionnaire used to study perceived workload to complete a task is composed of six questions on a Likert scale \cite{Hart1988}. The questions concern mental load, physical load, time pressure, expected success, effort required, and frustration experienced during the accomplishment of the task. The average value for the six questions constitutes a Raw Task Load indeX (RTLX) \cite{Byers1989} that we used to quantify how students feel when using learnr tutorials.


# Results

This study is performed all along the three courses that comprise 26 modules in 2020-2021. Table \ref {tab:tab_course} summarizes the number of H5P, learnr, individual and group GitHub projects that students had to complete. Group projects usually span over several modules. It should be noted that for course C, we also introduced a challenge in machine learning that replaced one group GitHub project. This challenge is omitted from the present analysis, being an isolate activity that is difficult to compare to the rest. However, this explains why there is only one group project in course C.

```{r tab_course_summary}
# Table of number of users and number of exercises by type ----------------------
read("../data/sdd_infos.csv") %>.%
  transmute(., Course = course, Students = user, Modules = module, H5P = h5p, Learnr = learnr,
    `Indiv. projects` = `ind. github`, `Group projects` = `group github`) %>.%
  knitr::kable(., format = "latex", caption = "\\label{tab:tab_course} Number of students, modules, and exercises for each course. For the learnr tutorials, the first number is the amount of tutorial documents and the second number in brackets is the total number of questions in these tutorials (year 2020-2021).")
```

Retrospective data from 2018-2019 (only course A) and 2019-2020 (course A and B) are also used when it is pertinent. For instance, final exams were only used during these two years. It should be kept in mind that the pedagogical material was written and improved progressively during the three years. The H5P exercises and the auto-checking of learnr answers were not available before 2020-2021. We do not use data corresponding to our older courses in biostatistics given in a more traditional way because we consider the comparison is not fair: the content of these courses is quite different. However, experience gathered with these courses during 15 years was critical during their redesign.


## Measured and perceived cognitive workload in learnr tutorials

In our courses, learnr tutorials play an essential role in the progressive acquisition of competences because they are at the transition between the theory (online book chapters) and the practice (projects where students deal with real biological data by themselves). These tutorials are interactive documents that recall main concepts, and take the students by the hand to perform their first data analysis step by step. At each step, they have at least one exercise or one quiz. The exercise consists in writing R code, or to fill missing parts in R code to progress in the analysis.

Our goal with these tutorials is to prepare the students optimally for the practice of data science. On the other hand, we do not want to exhaust their mental energy just before they start to work on their projects. The efficiency of these tutorials is qualitatively determined by observing the behavior of the students when they start their practical work, but we also have quantitative indicators available, like the number or retries necessary to complete an exercise on average, the number of exercises correctly answered, or the time needed to complete one tutorial.

A few tutorials were elaborated during the academic year 2018-2019, and positive feedback on their utility (both by direct observation of the students and thanks to their remarks) led us to systematize them into what we now call level 2 activities (see Table XX) in the form of learnr documents in 2019-2020. The tutorials were further refined in 2020-2021: we added contextual hints thanks to the gradethis R package. When students submit their answer to the exercises, the R code is analyzed and the result is compared with the solution. In case of differences, heuristics are used to provide contextual hints. Students can then refine their solution and resubmit it. This appears very efficient in self-teaching and self-assessing their competences before switching to the practice with confidence. 

```{r fig_learn_trials, out.width='100%', fig.cap="\\label{fig:fig_learn_trials} Average number of retries that where required for each student to find the right answer in learnr tutorials exercises (year 2020-2021). This measure is used as an indirect, but objective measurement of the cognitive workload. The black dot is the average for the whole classes and *n* is the number of observations."}

set.seed(87436)
read("../data/sdd_learnr.csv") %>.%
  chart(., l_trials_exercices ~ course) +
  geom_dotplot(binaxis = "y", stackdir = "center", binpositions = "all", fill = "lightgray") +
  #geom_boxplot(fill = "lightgray") +
  #geom_jitter(alpha = 0.5, width = 0.1) +
  stat_summary(fun.data = function(x) data.frame(y = max(x) * 1.1, label = paste("n =", length(x))), 
    geom = "text", hjust = 0.5) +
  stat_summary(fun = "mean", color = "black", size = 1) +
  labs(x = "Course", y = "Trials / learnr exercise") +
  ylim(2, 10.5)
```

```{r, results='hide'}
sdd_learnr <- read("../data/sdd_learnr.csv") %>.%
  mutate(., course = factor(course))
anova. <- lm(data = sdd_learnr, l_trials_exercices ~ course)
anova(anova.)
#bartlett.test(data = sdd_learnr, l_trials_exercices ~ course)
#plot(anova., which = 2)
summary(anovaComp. <- confint(multcomp::glht(anova.,
  linfct = multcomp::mcp(course = "Tukey"))))
rm(sdd_learnr)

# The qq plot est problématique
## soit on utilise un test non parématrique qui donne le même résultat statistique et on passe sur un boxplot.
# kruskal.test(data = sdd_learnr, l_trials_exercices ~ course)
# summary(kw_comp. <- nparcomp::nparcomp(data = sdd_learnr, l_trials_exercices ~ course))
```

The objective measurement of the cognitive workload based on the average number of entries that were required for each student to find the right answer in learnr tutorial exercises (Fig. \ref {fig:fig_learn_trials} ). This indirect measure varies significantly between the 3 courses (ANOVA, F(2,109) = 3.655, p-value = 0.029). The students in course C need significantly fewer trials to find the right answer than students in courses A (Tukey HSD, t = -2.489, p-value = 0.0375) and B (Tukey HSD, t = 0.0474, p-value = 0.047)

The perceived cognitive load required to perform these exercises is also a key aspect. This measure the emotional state of the students after having completed a tutorial. This has, as far as we know, not been studied yet. We used a NASA LTX questionnaire to assess it across all three courses. Participation to the survey was high: 48/59 (81%), 35/45 (78%) and 18/26 (69%) for courses A, B, and C respectively.

```{r fig_rtlx, out.width='100%', message=FALSE, warning=FALSE, fig.cap="\\label{fig:fig_rtlx} Perceived workload for the learnr tutorials in the three courses (year 2020-2021). The black circle is the mean RTLX value. The number above each box is the number of respondants."}
set.seed(253)
read("../data/sdd_rtlx.csv") %>.%
  chart(., rtlx ~ course) +
  geom_boxplot(fill = "lightgray") +
  geom_jitter(alpha = 0.5, width = 0.1) +
  labs(y = "RTLX", x = "Course") +
  stat_summary(fun = "mean", color = "black", size = 1) +
  stat_summary(fun.data = function(x) {
    data.frame(y = max(x) * 1.1, label = paste0("n = ", length(x)))
  }, geom = "text", hjust = 0.5) +
  labs( y = "Raw Task Load indeX")
```

The difficulty of the course, and thus, of the exercises in the tutorials increase from one course to the other. However, we do not observe an increase, neither in the number of retries, nor in the RTLX index (Fig. \ref {fig:fig_rtlx} ). On the contrary, these appear significantly lower for course C than for course A (ANOVA, F(2,98) = 3.588, p-value = 0.031; Tukey HSD, t = -2.679, p-value = 0.023). The cognitive load perceived by the students diminishes at the same time their ability to find the right answer more rapidly. This may be a consequence of a more fluent R coding and a better mastering of the software environment.

```{r rtlx_stats, results='hide'}
read("../data/sdd_rtlx.csv") %>.%
  mutate(., course = as.factor(course)) ->
  workload_rtlx
anova. <- lm(data = workload_rtlx, rtlx ~ course)
anova(anova.)
#bartlett.test(data = workload_rtlx, rtlx ~ course)
#plot(anova., which = 2)
summary(anovaComp. <- confint(multcomp::glht(anova.,
  linfct = multcomp::mcp(course = "Tukey"))))
rm(workload_rtlx)
```


## Final exam *versus* project

An exam at the end of an academic term is a common practice. So, we compare grading our students obtain from such an exam with score they obtain directly in their projects. The final exam is written in learnr, and it mixes questions on the theory with partly solved data analyses they have to explain, criticize and finalize during the exam session on the computer.

```{r fig_exams_projects, out.width='100%', fig.asp=0.4, fig.cap="\\label{fig:fig_exams_projects}  Grades obtained at the final exam in function of grades obtained for the projects for courses A and B during two years (course B was still in its old form in 2018-2019 and is thus not represented)."}
read("../data/sdd_eval.csv") %>.%
  mutate(., course_year = paste0(course, " (", acad_year, ")")) %>.%
  chart(., exam ~ result | course_year) +
  geom_vline(xintercept = 5, alpha = 0.3) +
  geom_hline(yintercept = 5, alpha = 0.3) +
  geom_jitter(alpha = 1, width = 0.05, height = 0.05, show.legend = FALSE) +
  ylim(c(0,10)) +
  xlim(c(0,10)) +
  labs(y = "Final exam grade (/10)", x = "Project grade (/10)") +
  theme(aspect.ratio = 1)
```

In 2018-2019 and 2019-2020, the summative assessment was based on the completion of a project and on a more conventional examination at the end of the term. The comparison of the grades obtained by each student for a project and a final exam shows only a weak correlation between these two types of evaluations [TODO: provide values here]. Year 2018-2019 marks the transition to a flipped classroom approach in our teaching of these data science courses. Only one student failed in the project, while almost one third of the same students failed their final exams. The difficulty of the project was similar to previous years, when the course was made of lectures followed by exercises (and when failure was not uncommon). The flipped classroom approach leaves more time in class to work on practical applications, to ask questions, to discuss results, ... We hypothesize that the very low failure rate could be explained by a better preparation to practical data analysis, but not to the final exam.

In 2019-2020, we raised a little bit the difficulty for the project, resulting in a more widespread distribution of the results, but with a similar pattern showing very little correlation between the two evaluation methods. The same conclusion can be drawn for course B, with several students failing in one of the two evaluations, but not in the other one.

Despite, a final examination that includes a series of practical questions (writing R code to analyse data, as in the projects), this type of assessment does not reflect the ability of the students to correctly process and analyse biological data. Alignment with intended learning outcomes seems thus to be more present in the projects. Due to these results, the final examination was abandoned for the year 2020-2021, and it has been replaced by a continuous evaluation of the students' activities across all four level exercises. These activities are analysed in the following section.


## Students activity profiles in continuous evaluation

Despite the fact that we have relatively homogeneous classes of students with similarly (low) level of knowledge in statistics and computing initially the flipped class approach and the proactive attitude we expect from them (they must formulate questions correctly whenever they face a problem), we observe they develop very different strategies. Not all students ask questions. Some of them try to find solutions on their own. Some other prefer to ask their questions privately, while others have no problems exposing their difficulties on a public forum (a Discord channel). The way and the timing they progress in the exercises also largely vary. The schedule is not tight and only suggest the rhythm of progression. No student is penalized if the exercises are done later, as soon as they are completed before the final deadline. We observe that some student prefer to stick to the proposed schedule, while others procrastinate and differ the completion of their exercises. Some strategies are more efficient than others. We analyzed traces from the students' activities to distinguish the various profiles and we correlate them with the grade they obtain at the end of the course.

In 2020-2021, to support the continuous evaluation method without a final exam, the course material was enriched with exercises organized into four increasing difficulty levels, as presented in Table XXX. The activity of the students in level 1 (H5P) and 2 (learnr) exercises is directly recorded in a database. For the GitHub projects (levels 3 and 4 exercises), it is the git log data that are analysed. During lockdown periods, exchange with students and answers to their questions were exclusively done by email, text or voice messages on Discord, either on private or public channels. Students were allowed to freely chose their favourite way to interact with the teachers and between each other. All these exchanges were recorded too. Finally, records in all activities were used to establish the final grade for the course. 
Final grade is the weighted average of the scores obtained at all four levels. The weight was adjusted from course to course according to the importance of the different projects, mainly. To give an idea, for course A second term, level 1 H5 P exercises accounted for 5%, 10% for level 2 learnr tutorials, 35% for level 3 individual projects and 40% for level 4 group works. More weight is always put on projects. On average, each student went through more than 130 assessments that accounted for the final grade. Two third of these assessments were established manually, using evaluation grids based on the reports they write using R Markdown [TODO: add a ref here] (.Rmd files, a literate programming system that allows including computations directly inside the report) in their projects. The remaining third is made of scores automatically calculated from the various online exercises.

For the three courses, we recorded more than 450,000 events, which makes on average almost 3,500 events for each student. These data contain information to characterize the behaviour and learning patterns that the students use. They are summarized into sixteen metrics.

For H5P exercises:

- trials/H5P ex.: the average number of trials for each H5P exercise (students can retry as much as they wish and they have immediate feedback if their answer is correct or not),

- correct H5P ex.: the fraction of H5P exercises that were correctly answered,

For learnr tutorial exercises:

- trials/learnr ex.: the average number of trials for each learnr exercise (here also, students can retry as much as they want),

- hints/learnr ex.: in learnr exercises, students can display hints to help them to solve the problems (but they lose 10% of the score of the exercise for each hint they view). This is the average number of hints per exercise that were displayed,

- correct learnr ex.: the fraction of learnr exercises that were completed with a correct answer,

- time/learnr ex.: the average time required to finish one learnr exercise involving R code writing.

For individual and group projects:

- commits/ind. projects: the average number of commits done by a student in one individual project,

- contributions/ind. projects: the number of lines changed (added or subtracted) in an R Markdown report by one student in one individual project, on average,

- commits/group projects: same as above, but for group projects,

- contributions/group projects: same as above, but for group projects,

- percentage of contributions to group projects: the fraction of work the student did, relative to all the work done in group projects (still only counted for .Rmd files as the number of lines changed from one version to the other).

For support:

- questions/module: the number of questions student asked, divided by the number of modules in the course,

- percent of public questions: the fraction of questions that the student posted in a public channel (a channel dedicated to the course that all the other students of the class can read),

- contributions/question: a metric that catches the relative "productivity" of the student related to the number of questions they ask.

Finally, global measurements:

- work done: the fraction of all exercises that the student finished,

- work done in time: the fraction the exercises done in the right time, that is, during the proposed calendar.

In our courses, we have a few students in mobility that come from various origins. The a priori knowledge is important in education. So, to avoid biases due to the past curriculum of the students, we restrict this analysis to the subpopulation that comes from the first year of Bachelor in Biology at UMONS only. A Kohonen's self-organizing map is used to create student profiles according to their activities, see Fig. XXX. A 3x3 hexagonal cells pattern was chosen, and students are thus classified into nine different groups.

```{r som, echo=FALSE}
sdd_metrics <- suppressMessages(read("../data/sdd_metrics.csv"))
metrics_nograde <- select(sdd_metrics, -user, -course, -state, -grade) # Only metrics

# Perform the SOM now
library(kohonen)
set.seed(9433)
metrics_nograde %>.%
  scale(.) %>.% # Standardization of the metrics
  as.matrix(.) %>.% # supersom() needs a matrix
  supersom(., grid = somgrid(3, 3, topo = "hexagonal"),
    rlen = 100L, mode = "online") ->
  sdd_som

# plot.kohonen() does not allow to specify the margins => customize this!
plot_som <- function(x, classif = NULL, main = "", labels = NULL, pchs = NULL,
  bgcol = NULL, keepMargins = FALSE, shape = c("round", "straight"),
  border = "black", margins = rep(0.6, 4), ...) {
  if (is.null(main))
    main <- "Mapping plot"
  #margins <- rep(0.6, 4)
  if (main != "")
    margins[3] <- margins[3] + 2
  if (!keepMargins) {
    opar <- par("mar")
    on.exit(par(mar = opar))
  }
  par(mar = margins)
  if (is.null(classif) & !is.null(x$unit.classif)) {
    classif <- x$unit.classif
  } else {
    if (is.list(classif) && !is.null(classif$unit.classif))
      classif <- classif$unit.classif
  }
  if (is.null(classif))
    stop("No mapping available")
  kohonen:::plot.somgrid(x$grid, ...)
  title.y <- max(x$grid$pts[, 2]) + 1.2
  if (title.y > par("usr")[4] - 0.2) {
    title(main)
  } else {
    text(mean(range(x$grid$pts[, 1])), title.y, main, adj = 0.5,
      cex = par("cex.main"), font = par("font.main"))
  }
  if (is.null(bgcol))
    bgcol <- "transparent"
  shape <- match.arg(shape)
  sym <- ifelse(shape == "round", "circle", ifelse(x$grid$topo ==
      "rectangular", "square", "hexagon"))
  switch(sym,
    circle = symbols(x$grid$pts[, 1], x$grid$pts[, 2],
      circles = rep(0.5, nrow(x$grid$pts)), inches = FALSE,
      add = TRUE, fg = border, bg = bgcol),
    hexagon = kohonen:::hexagons(x$grid$pts[,
      1], x$grid$pts[, 2], unitcell = 1, col = bgcol, border = border),
    square = symbols(x$grid$pts[, 1], x$grid$pts[, 2],
      squares = rep(1,
        nrow(x$grid$pts)), inches = FALSE, add = TRUE, fg = border,
      bg = bgcol))
  if (is.null(pchs))
    pchs <- 1
  if (is.null(labels) & !is.null(pchs))
    points(x$grid$pts[classif, 1] + rnorm(length(classif),
      0, 0.12), x$grid$pts[classif, 2] + rnorm(length(classif),
        0, 0.12), pch = pchs, ...)
  if (!is.null(labels))
    text(x$grid$pts[classif, 1] + rnorm(length(classif),
      0, 0.12), x$grid$pts[classif, 2] + rnorm(length(classif),
        0, 0.12), labels, ...)
  invisible()
}

# How variables explain the distribution?
grays <- function(n, ...) {
  gray.colors(n, start = 1, end = 0, ...)
}
vars <- names(metrics_nograde)
var_names <- c(
  question     = "questions/module",
  `q_pub%`     = "% public questions",
  q_prod       = "contributions/question",
  `h_ok%`      = "correct H5P ex.",
  h_trials     = "trials/H5P ex.",
  `l_ok%`      = "correct learnr ex.",
  l_trials     = "trials/learnr ex.",
  l_hints      = "hints/learnr ex.",
  l_time       = "time/learnr ex.",
  i_commits    = "commits/ind. project",
  i_changes    = "contributions/ind. project",
  g_commits    = "commits/group project",
  g_changes    = "contributions/group project",
  `g_contrib%` = "% contribution to group projects",
  `done%`      = "work done",
  `intime%`    = "work done in time"
  )
```

```{r fig_som, out.width='100%', fig.cap="\\label{fig:fig_som} Self-organizing map of the student activities across the three courses (year 2020-2021). See the text for the explanations."}
set.seed(3749)

# First draw the 10 little explanatory plots
cex_main <- 0.8
par(mfrow = c(5, 4), mar = rep(0.4, 4))
iter_letters <- iterators::iter(LETTERS)

# Row 1
for (var in vars[c(7, 8, 6, 15)]) {
  plot(sdd_som, type = "property",
    property = sdd_som$codes[[1]][, var],
    shape = "straight", main = "", palette.name = grays, heatkey = FALSE)
  title(paste0("\n", iterators::nextElem(iter_letters), ". ", var_names[var]),
    cex.main = cex_main)
}

# Row 2
plot.new(); plot.new(); plot.new(); plot.new()

# Row 3
var <- vars[1]
plot(sdd_som, type = "property",
  property = sdd_som$codes[[1]][, var],
  shape = "straight", main = "", palette.name = grays, heatkey = FALSE)
title(paste0("\n", iterators::nextElem(iter_letters), ". ", var_names[var]),
  cex.main = cex_main)

plot.new(); plot.new()

var <- vars[13]
plot(sdd_som, type = "property",
  property = sdd_som$codes[[1]][, var],
  shape = "straight", main = "", palette.name = grays, heatkey = FALSE)
title(paste0("\n", iterators::nextElem(iter_letters), ". ", var_names[var]),
  cex.main = cex_main)

# Row 4
plot.new(); plot.new(); plot.new(); plot.new()

# Row 5
for (var in vars[c(2, 16, 10, 12)]) {
  plot(sdd_som, type = "property",
    property = sdd_som$codes[[1]][, var],
    shape = "straight", main = "", palette.name = grays, heatkey = FALSE)
  title(paste0("\n", iterators::nextElem(iter_letters), ". ", var_names[var]),
    cex.main = cex_main)
}

# Then, plot the central item
cols <- scales::alpha(c(A = "red", B = "purple1", C = "blue"), 0.75)

par(mfrow = c(1, 1), new = TRUE)
font_cex <- 0.7
plot_som(sdd_som, shape = "straight", col = cols[as.factor(sdd_metrics$course)],
  pch = 19, cex = (sdd_metrics$grade / 60) + 0.3, main = " ", keepMargins = TRUE,
  margins = rep(2.7, 4))
# Add numbers to cells
text((1:3) + 0.5, 3, 1:3, cex = font_cex)
text(1:3, 2.15, 4:6, cex = font_cex)
text((1:3) + 0.5, 1.3, 7:9, cex = font_cex)
legend(1.5, 3.65, legend = c("A", "B", "C"), col = cols, pch = 19,
  horiz = TRUE, bty = "n", title = "Course", cex = font_cex)

rm(sdd_metrics, metrics_nograde, grays, plot_som)
```

In \ref {fig:fig_som}, the small plots in gray show how selected metrics distribute in the nine cells, from lowest value in white to highest value in black. They help to decrypt the way students behave according to their profile. Metrics that are not represented present similar patterns than others (for instance H5P metrics exhibit a similar pattern as learnr metrics and therefore, they are not represented). Dots in the central plot are the various students, with colour representing the course and the diameter of the dots representing the grade the students obtained at the end of the course. The following paragraphs detail information in that figure. The numbers between brackets mean the cell number in the central plot, and the upper case letters in brackets refer to the peripheral subplots.

Although most students finished all, or almost all exercises (D), cell (3) collects the few students that did only a tiny part of the exercises. These students obtained very low grades, of course. They belong to courses A and B. On the other hand, heavy workers are at the bottom (I & J), and good performers in learnrs (C) are in cells (5-9).

- Besides the absent students of cell (3), cells (2 and 6) collect students that seldom ask questions (E), and that rarely appear on the public channel (G). Minor differences separate them. For instance, cell (2) sometimes use learnr hints (B), while cell (6) never does, also because they find the correct answer to the exercises more often by themselves (C). Asking questions is at the core of our pedagogical approach. So, these students do not play the game. However, they can possibly succeed. Some of them probably exchange with other students through different channels that we do not monitor. Cell (2) -more difficulties with learnr tutorials- mainly contains students belonging to course, A, while cell (6) contains students of courses B and C. There is a clear evolution in their behaviour from one course to the other in term of ease in front of the exercises, even if they remain silent in term of teacher interactions.

- Among the students that have a hard time to figure out the answers to auto-evaluation exercises, cell (1) reassemble people that most heavily rely on learnr hints (B), and are among those who need to retry those exercises more often before figuring out the correct answer (A), a characteristic they share with cell (4). These students also ask a lot of questions (E), both on the public and private channels (G, mid gray). Main difference between those two groups is that students in cell (4) try harder to find the answer without looking at the hints, while in cell (1) they give up more rapidly. Also these students respect the proposed schedule much more closely than all others (H). We have students in all courses there, but a majority from course A. 

- All these cells (1-4 plus 6) are students that exhibit suboptimal behaviours in one or the other way. The remaining cells (5 & 7-9) correspond to students that perform better from this point of view. Cell (5) is primarily represented by students from course A, but otherwise, also from course B and C. These are average actors in all categories, except they are fluent with level 1 (H5P, not shown) and level 2 (learnr, C) exercises.

- Moving from cell (5) to (7), (8) and (9), we encounter increasingly top performers. The number of students from course A becomes progressively lower, while course B, and especially C dominate in these groups. Cell (7) use largely the public channel (G) and respect the schedule quite well (H) as main difference from those from cell (5). Students in cells (8) and (9) are not so much in time, but this is because they are heavier workers in the projects, both in the individuals (I) and in the groups (J) activities. This needs obviously more time. In cell (9) we also have the students that contribute the most to the reports in terms of lines added or deleted (F).

To summarise, at the top of the SOM, cells (1-4, plus 6) contain students with not optimal behaviour, cell (5) are average students, and cells (7-9) at the bottom exhibit profiles corresponding to best performers. The pattern is also visible between courses A (mainly distributed at the top or centre) to B and C (more represented at the bottom). This suggests that students need time to get used to the course, its pedagogical approach, and/or the software environment they have to use.


## Transition between face-to-face and distance learning

Due to Covid-19 lockdown periods, distance learning had to be adopted abruptly. We analyse the activity collected during academic years 2019-2020 and 2020-2021 to evaluate the impact of these transitions on the progression of the students In Fig. XXX, the academic term is divided here into seven work periods of approximately two weeks (remind that it is the rhythm of the courses: one module every second week). The classes of the second term of the year 2019-2020 start at period Y1P09, since period Y1P08 is reserved for the exams. The courses of the first term of 2020-2021 begins at Y2P01. First lockdown started at period Y1P11 for one month and an half. Second lockdown stared at Y2P03 and lasted at then end of the second term (Y2P15). During the first lockdown, we rapidly opened the dedicated Discord channels and a common email address for all teachers were installed for a faster reaction.

```{r fig_support_by_time, out.width='100%', message=FALSE, fig.asp=0.9, fig.cap="\\label{fig:fig_support_by_time} a. Contributions of the students to the projects by periods of two weeks of course. b. Contributions by question asked (log scale) as a proxy measurement of the effect of student-teacher interactions on the progression in data analysis. Light gray background indicates periods where distance teaching was mandatory due to Covid-19 lockdown (Y1 is 2019-2020, Y2 is 2020-2021)."}
log_period <- read("../data/log_period.csv")
support_period <- read("../data/support_period.csv")

# First plot
log_period %>.%
  filter(., !period %in% c("Y1P15", "Y1P16", "Y1P17")) %>.%
  group_by(., period, .modules) %>.%
  summarise(.,
    change = sum(change),
    nus = length(unique(user)),
    change_user = change / nus) %>.%
  chart(data = ., change_user ~ as.factor(period) %fill=% as.factor(.modules)) +
  geom_col(col = "black") +
  annotate("rect", xmin = 1.5, xmax = 4.5, ymin = -20, ymax = 1000, alpha = .4, fill = "lightgray") +
  annotate("rect", xmin = 6.5, xmax = 19.5, ymin = -20, ymax = 1000, alpha = .4, fill = "lightgray") +
  geom_col(col = "black") +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_fill_grey(start = 1, end = 0.2) +
  labs( y = "Contributions / student", fill = "Number of modules by period") +
  scale_y_continuous(
    breaks = c(0, 250,500, 750),
    labels = c(0, 0250, 500, "  750")) +
  theme(
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank()) ->
  p1

# Second plot
inner_join(log_period, support_period) %>.%
  # Calculate ratio support/production as messages/additions
  mutate(.,
    add_by_message = add/messages,
    change_by_message = change/messages) %>.%
  chart(., change_by_message ~ as.factor(period) %fill=% as.factor(.modules)) +
  #geom_vline(xintercept = c("Y1P11", "Y2P03"), alpha = 0.3, linetype = "twodash") +
  geom_boxplot() +
  annotate("rect", xmin = 1.5, xmax = 4.5, ymin = 0, ymax = 15000, alpha = .4, fill = "lightgray") +
  annotate("rect", xmin = 6.5, xmax = 19.5, ymin = 0, ymax = 15000, alpha = .4, fill = "lightgray") +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), labels = c(0.1, 1, 10, 100, 1000)) +
  stat_summary(fun.data = function(x)
    c(y = max(x) + 0.5, label = length(x)), geom = "text", hjust = 0.5) +
  scale_fill_grey(start = 1, end = 0.2) +
  labs(y = "Contributions / question", x = "Period", fill = "Number of modules") ->
  p2

# Combined plot
combine_charts(list(p1, p2), nrow = 2, common.legend = TRUE, legend = "bottom",
  heights = c(0.9, 1), labels = "auto", font.label = list(size = 12))

rm(log_period, support_period, p1, p2)
```

[TODO: this paragraph must be adapted and reworked!] The contribution to the reports remains relatively proportional to the number of questions the students sent by email or Discord messages, no matter the period and the intensity of the work as indicated by the number of modules to be completed during the period, all three courses pooled together. Only the number of students that ask questions in these channels changes between face-to-face and distance teaching (much less in face-to-face because most of the students ask their questions directly in the classroom). Transition from direct interaction to electronic exchange was quasi immediate during the lockdown. Consequently, support provided by the teachers in distance learning ... to be continued.


# Discussion

Teaching data science to a population of students that are not very used to advanced computer techniques and tools, and that have only basic knowledge in mathematics and statistics is a hard task \cite{Sousa2018}. Our basic approach is to extend the training on a very long period: five successive terms spanning on three consecutive years (undergraduate and graduate). That way, the many different concepts they have to learn can be broken down into subunits (26 modules) that last for two weeks each. We also used highly developed flipped classrooms and blended teaching and learning (following Spadafora & Zopito’s definition of “any educational model where online delivery ranges from 50% to 80% [TODO: add citation here]), with an emphasis on proactive exchange with the teachers: students have to ask questions to progress.

[add IDEA  : \cite{Compeau2019} montre les bon résultats abtenu avec la classe inversée et les challenge en biologie lors de son cours de 2018.]

Our students are more used to a traditional approach made of lectures followed by exercises where important concepts are repeated at the beginning of the practical sessions. They tend to have a passive attitude during lectures and they expect the teachers and assistants feed them with the key concepts. That attitude does not purposely work here. Proactive behavior and developing autonomy are required \cite{Freeman2014}. They thus have to engage themselves in a very different way of learning. The transition between the theory they read in the book and the projects where they have to apply these concepts is too abrupt without a progression in four stages: (1) auto-evaluation exercises directly in the book, (2) recall of the main concepts and guided step-by-step analysis of a first dataset with the learnr tutorials and (3) at least one guided individual project with another dataset, before (4) they are presented yet another dataset, with limited instructions this time. Task two, the learnr tutorial, was immediately spotted by our students as a key activity in 2018-2019. So, we have focused our attention on these documents. In 2020-2021, we have also added an heuristic engine (gradethis) to provide contextual feedback on the errors students make in their answers. The RTLX index measured in 2020-2021 will serve in the future as a reference to work towards correctly designed tutorials, with lower perceived workload without sacrificing the content. The significant decrease in RTLX value from course A to C indicates that there is a still a margin of progression. We would like to observe such a decrease sooner, perhaps already in the second course. It is indeed important "to maintain reflective and systematic approaches in both the development and evaluation [of our] blended approach" [TODO: cite Spadafora & Zopito].

Ultimately, the task that better evaluates their practical skills in biological data analysis, and which meets with the courses' outcomes, is the group project because students have to demonstrate what they can do in situation: complex dataset, minimal instructions. They have to figure out a suitable question, analyze the data to answer that question, present their results in a report and discuss what they found with a critical mind \cite{Auker2020}. This task is complex, especially for undergraduate students. That is why it is run in groups of two to four students, depending on the complexity of the problem. Here, they mobilize their collective intelligence to get results many of them would be hardly capable of achieving alone. The tracking of individual activity in the project thanks to git allows figuring out clearly what was the contribution of each student in the group and to score their individual contributions suitably.

Sometimes, groups do not work well, and one student has to do most of the work. This is a clear weakness in this approach, especially if one of the students in Fig. \ref {fig:fig_som} cell (3) is involved. If we could identify the profile of the different students relatively early during the course, we would be able to create better grouping of the students with a blend of different complimentary profiles to enrich their experience. Maybe should we work exclusively with groups of four to mitigate the impact of one defeating student?

We observed a low correlation between performances in group projects and in grades obtained at final exams (Fig. \ref {fig:fig_exams_projects} ). This led us to stop using final exams, at the benefit of a continuous evaluation process with emphasis into group projects. We had to set up a procedure to monitor and score the students' activity in all the exercises by automatic scoring online exercises, and to manually score all projects against an evaluation grid, in the light of the different versions tracked by git. This is time-consuming despite the partially automatic scoring lowers the charge. It would be interesting to investigate whether tools derived from learning analytics and computing science could second teachers in this work. For instance, reproducible research is one of the competences our students have to develop. It implies that their R Markdown documents should compile into reports in HTML or PDF format without any error. This criterion could be checked automatically.

[IDEA TO ADD in discussion : Nous pouvons utilisez l'ensemble des données collectée afin de réaliser learning analytics qui est une discipline en plein essort. Elle ne se limite pas à la prédiction des résultats des étudiants. \cite{Siemens2013} ]

Activity tracking in the exercises, primarily set up for the continuous evaluation, also offers the opportunity to study the way learning happens (or not). Learning analytics are primarily used to early predict success or failure [TODO: add refs here]. In our courses, failure rate is already rather low and essentially limited to a few defeating students that do not work at all. We are more interested to classify our participating students according to their behaviors. This paves the way toward a more inclusive pedagogy by spotting different kinds of suboptimal patterns (for instance, never asking questions, looking at hints too quickly without really trying to figure out the answer, being shy to discuss problems on public channels, ...) Once these patterns are evidenced, we can think of countermeasures. As an example for students that rarely post their questions publicly, we will test an alternate discussion channels were teachers never post but have read access. In case of an error, the teacher will contact the student privately to explain him what is wrong. That student would then have to reexplain himself to its siblings. This way, its error is never publicly spotted by the teacher. This approach was very successful with our so-called "eleve-assitants" -students from higher classes that have brilliantly succeeded in the course one or two years ago and that second the teaching staff-. With tools like the self-organizing map we should be able to predict suboptimal student profiles early. We could engage a discussion with the concerned persons to determine the cause and find a solution as early as possible. Learning analytics used that way would promote a differential pedagogical approach, a key for more inclusive teaching [ref needed here].

Emotional state of the students and motivation are also very important. Validated questionnaires, like NASA-LTX, or [ .... complete here] allow us to assess these aspects.

[I still have to write a paragraph that discusses the results regarding the timing events around covid-19 lockdown periods and their implications....]


# Conclusion

Teaching data science comes with challenges. The discipline is quite young, and we still are seeking the best pedagogical approach. After three years of teaching data science to undergraduate and graduate students in a curriculum in biology with revised pedagogical practices, we have our first cohort that has followed all three courses. There are still two optional courses available in second year of the Master if they want to push their data science skills further on. However, the three mandatory courses are designed to be sufficient by themselves. Globally, most students acquired the competences during these courses. We have the feeling that they are more mature and more capable in data science than with our previous courses in biostatistics given in a more traditional way. The impact of the revised approach to teach biological data science on the way learners manage data and data analysis will be observable during the following years. We will monitor how these students apply their skills in their Master thesis, and later, in their career or during their PhD thesis. Meanwhile, we will continue to improve our courses by further exploiting the data we accumulate on the activity of our students. Experience gathered during forced distance learning during Covid-19 lockdown will be used too to improve our courses. The radical changes that were required in that context showed that students can accommodate to a large extent, but also that a diversification of the activities is beneficial [TODO: citer Spadofora et Marini 2018... ou autre car déjà beaucoup cité dans le papier]. Speaking about diversification, in 2020-2021 we have successfully tested a kaggle-like challenge (<https://www.kaggle.com/competitions>) in one of the machine learning modules. Such ludic activities would also contribute to the diversification of pedagogical practices, interest and motivation of the students [TODO: refs needed if possible]. We would also be happy to share experience with other teachers in data science. Altogether, we are on the way to reshape the post-covid teaching landscape, and it will probably be quite different to what we are used today!
